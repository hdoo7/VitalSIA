{"version":3,"file":"static/js/503.a108b4bf.chunk.js","mappings":"uHAAe,MAAMA,EACjBC,WAAAA,GAA4C,IAAhCC,EAAeC,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,YAC1BG,KAAKC,YAAc,KACnBD,KAAKE,eAAgB,EACrBF,KAAKG,mBAAoB,EACzBH,KAAKI,gBAAgBR,EACzB,CAEAQ,eAAAA,CAAgBR,GACY,cAApBA,GACAI,KAAKC,YAAc,IAAKI,OAAOC,mBAAqBD,OAAOE,yBAC3DP,KAAKC,YAAYO,gBAAiB,EAClCR,KAAKC,YAAYQ,YAAa,EAC9BT,KAAKC,YAAYS,KAAO,SAExBC,QAAQC,MAAM,+BAEtB,CAGAC,0BAAAA,CAA2BC,GACnBd,KAAKE,eAELF,KAAKC,YAAYc,MAAQ,KACrBf,KAAKE,eAAgB,EACrBF,KAAKgB,wBAAwBF,EAAqB,EAEtDd,KAAKiB,oBAGLjB,KAAKG,mBAAoB,EACzBH,KAAKgB,wBAAwBF,GAErC,CAGAE,uBAAAA,CAAwBF,GACpB,GAAId,KAAKE,cAEL,YADAS,QAAQO,KAAK,mDAIjBlB,KAAKE,eAAgB,EACrB,MAAMiB,EAAkB,GAExBnB,KAAKC,YAAYmB,SAAYC,IACzB,MAAMC,EAAUD,EAAMC,QACtB,IAAK,IAAIC,EAAIF,EAAMG,YAAaD,EAAID,EAAQxB,OAAQyB,IAC5CD,EAAQC,GAAGE,SACXN,EAAgBO,KAAKJ,EAAQC,GAAG,GAAGI,WAAWC,QAGtDd,EAAqBK,EAAgBU,KAAK,KAAK,EAInD7B,KAAKC,YAAY6B,QAAWT,IACJ,gBAAhBA,EAAMT,OACND,QAAQC,MAAM,yDACdmB,MAAM,yEACN/B,KAAKiB,mBACkB,cAAhBI,EAAMT,OACbD,QAAQO,KAAK,0CAERlB,KAAKG,mBACN6B,YAAW,KACPrB,QAAQsB,IAAI,0DACZjC,KAAKgB,wBAAwBF,EAAqB,GACnD,OAGPH,QAAQC,MAAM,qBAAsBS,EAAMT,OAC1CZ,KAAKiB,kBACT,EAGJjB,KAAKC,YAAYc,MAAQ,KACrBJ,QAAQsB,IAAI,6BACZjC,KAAKE,eAAgB,EAChBF,KAAKG,oBACNQ,QAAQsB,IAAI,oCACZjC,KAAKgB,wBAAwBF,GACjC,EAGJd,KAAKC,YAAYiC,QACjBvB,QAAQsB,IAAI,8BAChB,CAGAhB,eAAAA,GACQjB,KAAKE,gBACLF,KAAKG,mBAAoB,EACzBH,KAAKC,YAAYkC,OACjBxB,QAAQsB,IAAI,wCACZjC,KAAKE,eAAgB,EAE7B,E,8FC9FJ,MA+BA,EA/B8BkC,IAAiB,IAAhB,OAAEC,GAAQD,EAOvC,OACEE,EAAAA,EAAAA,KAACC,EAAAA,GAAG,CACFC,SAAS,QACTC,OAAO,OACPC,OAAO,OACPC,KAAK,OACLC,GAAG,WACHC,aAAa,KACbC,EAAG,EACHC,UAAU,KACVC,WAAW,OAAMC,UAEjBC,EAAAA,EAAAA,MAACX,EAAAA,GAAG,CAACY,QAAQ,OAAOC,cAAc,SAASC,WAAW,SAAQJ,SAAA,EAC5DX,EAAAA,EAAAA,KAACgB,EAAAA,EAAI,CAACC,GAAIC,EAAAA,IAAUC,MAnBX,CACbC,QAAS,UACTC,UAAW,YACXC,KAAM,cAgBgCvB,GAASwB,QAAS,EAAGC,GAAI,KAC3DZ,EAAAA,EAAAA,MAACa,EAAAA,EAAI,CAACN,MAAM,QAAQO,WAAW,OAAMf,SAAA,CACvB,YAAXZ,GAAwB,UACb,cAAXA,GAA0B,YACf,SAAXA,GAAqB,cAGtB,C,kDC3BV,MA+FA,EA/FiB4B,CAACC,EAAaC,EAAcC,EAAqBC,EAAgBC,KAC9E,MAAOC,EAAmBC,IAAwBC,EAAAA,EAAAA,UAAS,CACvDpC,OAAQ,OACRqC,gBAAiB,KACjBC,aAAc,OAIZC,GAA0BC,EAAAA,EAAAA,QAAOP,KAAsBQ,QACvDC,GAAsBF,EAAAA,EAAAA,QAAOR,KAAkBS,QAE/CE,GAAqBC,EAAAA,EAAAA,cAAYC,UAC9BC,GAAwB,KAAhBA,EAAKvD,QASlB4C,GAAsBY,IAAI,IACnBA,EACH/C,OAAQ,UACRsC,aAAcQ,YAGZhB,EAAakB,YAAYF,GAE/BX,GAAsBY,IAAI,IACnBA,EACH/C,OAAQ,YACRsC,aAAc,SAElBW,KArBId,GAAsBY,IAAI,IACnBA,EACH/C,OAAQ,YACRsC,aAAc,QAkBN,GACjB,CAACR,IAEEoB,GAAyBN,EAAAA,EAAAA,cAAaP,IACxCF,GAAsBY,IAAI,IACnBA,EACH/C,OAAQ,WACRqC,sBAIJ,MAAQc,MAAOC,GAAab,EAAwBc,KAAKhB,GAErDe,GACAT,EAAmBS,GAAUE,MAAK,KAC9B,MAAQH,MAAOI,GAAiBb,EAAoBW,OAChDE,EACAZ,EAAmBY,GAEnBpB,GAAsBY,IAAI,IACnBA,EACH/C,OAAQ,OACRsC,aAAc,QAEtB,GAER,GACD,CAACK,EAAoBJ,EAAyBG,IAE3CO,GAAiBL,EAAAA,EAAAA,cAAY,KAC/BT,GAAsBY,IAAI,IAAWA,EAAM/C,OAAQ,gBAEnD6B,EAAYrD,4BAA4B6D,IACpCa,EAAuBb,EAAgB,GACzC,GACH,CAACR,EAAaqB,IAuBjB,MAAO,CAAEhB,oBAAmBsB,mBArBFZ,EAAAA,EAAAA,cAAYC,UAClC,MAAQM,MAAOM,GAAkBf,EAAoBW,OAErDlB,GAAsBY,IAAI,IACnBA,EACH/C,OAAQ,UACRsC,aAAcmB,YAGZ3B,EAAakB,YAAYS,GAE/BtB,GAAsBY,IAAI,IAAWA,EAAM/C,OAAQ,YAAasC,aAAc,SAC9EW,GAAgB,GACjB,CAACnB,EAAcmB,EAAgBP,IAQagB,kBANtBd,EAAAA,EAAAA,cAAY,KACjCf,EAAYjD,kBACZkD,EAAa6B,aACbxB,EAAqB,CAAEnC,OAAQ,OAAQqC,gBAAiB,KAAMC,aAAc,MAAO,GACpF,CAACT,EAAaC,IAEgD,C,0ICxCnE,QAvDF,MACIxE,WAAAA,CAAYsG,GACV,IAAKA,GAA4B,kBAAXA,EACpB,MAAM,IAAIC,MAAM,4CAElBlG,KAAKiG,OAASA,EACdjG,KAAKmG,OAAS,4CAChB,CAQA,iBAAMC,CAAYjB,GAChB,MAAMkB,EAAU,CACdC,MAAO,gBACPC,SAAU,CACR,CAAEC,KAAM,SAAUC,QAJW5G,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,oCAKhC,CAAE2G,KAAM,OAAQC,QAAStB,IAE3BuB,WAAY,GACZC,YAAa,KAGf,IAAK,IAADC,EAAAC,EAAAC,EAAAC,EACF,MAAMC,QAAiBC,MAAMjH,KAAKmG,OAAQ,CACxCe,OAAQ,OACRC,QAAS,CACP,cAAgB,UAADC,OAAYpH,KAAKiG,QAChC,eAAgB,oBAElBoB,KAAMC,KAAKC,UAAUlB,KAGvB,IAAKW,EAASQ,GACZ,MAAM,IAAItB,MAAM,mCAADkB,OAAoCJ,EAAS3E,SAG9D,MACMoF,EAA0B,QAAfb,SADEI,EAASU,QACHC,eAAO,IAAAf,GAAK,QAALC,EAAZD,EAAe,UAAE,IAAAC,GAAS,QAATC,EAAjBD,EAAmBe,eAAO,IAAAd,GAAS,QAATC,EAA1BD,EAA4BL,eAAO,IAAAM,OAAvB,EAAZA,EAAqCnF,OAEzD,IAAK6F,EACH,MAAM,IAAIvB,MAAM,+CAGlB,OAAOuB,CACT,CAAE,MAAO7G,GAEP,MADAD,QAAQC,MAAM,kCAAmCA,GAC3CA,CACR,CACF,G,eC3CJ,IAAIuD,EACAD,EACA2D,EACAC,EAAO,KAEX,MAUa5F,EAAQgD,MAAO6C,EAAkBC,EAAaC,KACvD,IAAKA,IAAiBA,EAAanD,QAE/B,YADAnE,QAAQC,MAAM,oCAZIsE,OAAO6C,EAAkBC,KAC/C,MAAM,OAAE/B,EAAM,eAAEiC,GAAmBF,EAEnC9D,EAAc,IAAIxE,EAAAA,EAAY,aAC9ByE,EAAegE,EAAAA,EAAaC,YAAYL,GACxCF,EAAgB,IAAIQ,EAAoBpC,SAElC9B,EAAamE,gBAAgBJ,GAAkB,UAAU,EASzDK,CAAkBR,EAAkBC,GAE1C,MAAMQ,EAAUA,KACZ,MAAM,kBAAEjE,EAAiB,kBAAEsB,EAAiB,iBAAEE,IAAqB9B,EAAAA,EAAAA,GAASC,EAAaC,EAAc0D,GACjGY,GAAQC,EAAAA,EAAAA,KAyBd,OAvBAC,EAAAA,EAAAA,YAAU,KACN9C,IACO,IAAME,MACd,CAACF,EAAmBE,KAEvB4C,EAAAA,EAAAA,YAAU,KAC2B,aAA7BpE,EAAkBlC,OAClBoG,EAAM,CACFG,MAAO,gBACPC,YAAa,8BACbxG,OAAQ,OACRyG,SAAU,MAEsB,YAA7BvE,EAAkBlC,QACzBoG,EAAM,CACFG,MAAO,WACPC,YAAatE,EAAkBkD,YAC/BpF,OAAQ,UACRyG,SAAU,KAElB,GACD,CAACvE,EAAkBlC,OAAQkC,EAAkBkD,YAAagB,KAEtDnG,EAAAA,EAAAA,KAACyG,EAAAA,EAAqB,CAAC1G,OAAQkC,EAAkBlC,QAAU,EAGjEyF,IACDA,EAAOkB,EAAAA,WAA0Bf,EAAanD,UAGlDgD,EAAKmB,QAAO3G,EAAAA,EAAAA,KAACkG,EAAO,IAAI,EAGfrG,EAAOA,KACZ+B,GACAA,EAAYjD,kBAEZkD,GACAA,EAAa6B,aAEb8B,IACAA,EAAKoB,UACLpB,EAAO,KACX,C","sources":["VISOS/perception/audio/AudioToText.js","components/TrafficLightIndicator.jsx","hooks/useConvo.js","VISOS/cognition/TextToGptReconciler.js","modules/chat.js"],"sourcesContent":["export default class AudioToText {\n    constructor(recognitionType = 'webspeech') {\n        this.recognition = null;\n        this.isRecognizing = false;\n        this.isManuallyStopped = false; // New flag to track manual stopping\n        this.initRecognition(recognitionType);\n    }\n\n    initRecognition(recognitionType) {\n        if (recognitionType === 'webspeech') {\n            this.recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();\n            this.recognition.interimResults = false;\n            this.recognition.continuous = true;\n            this.recognition.lang = 'en-US';\n        } else {\n            console.error(\"Unsupported recognition type\");\n        }\n    }\n\n    // Start recognition, but first stop any existing recognition\n    startContinuousRecognition(onRecognizedCallback) {\n        if (this.isRecognizing) {\n            // Recognition is already running, so stop it first\n            this.recognition.onend = () => {\n                this.isRecognizing = false;\n                this.startRecognitionProcess(onRecognizedCallback); // Start the recognition after stopping\n            };\n            this.stopRecognition();\n        } else {\n            // Start recognition directly if it is not already running\n            this.isManuallyStopped = false; // Reset manual stop flag\n            this.startRecognitionProcess(onRecognizedCallback);\n        }\n    }\n\n    // Process for starting recognition\n    startRecognitionProcess(onRecognizedCallback) {\n        if (this.isRecognizing) {\n            console.warn(\"Recognition is already running, skipping start.\");\n            return; // Avoid starting if it's already running\n        }\n\n        this.isRecognizing = true;\n        const finalTranscript = [];\n\n        this.recognition.onresult = (event) => {\n            const results = event.results;\n            for (let i = event.resultIndex; i < results.length; i++) {\n                if (results[i].isFinal) {\n                    finalTranscript.push(results[i][0].transcript.trim());\n                }\n            }\n            onRecognizedCallback(finalTranscript.join(' ')); // Send the transcription result\n        };\n\n        // Handle recognition errors\n        this.recognition.onerror = (event) => {\n            if (event.error === 'not-allowed') {\n                console.error(\"Recognition error: Microphone access was not allowed.\");\n                alert(\"Please allow microphone access to use the speech recognition feature.\");\n                this.stopRecognition(); // Stop recognition on permission error\n            } else if (event.error === 'no-speech') {\n                console.warn(\"Recognition error: No speech detected.\");\n                // Automatically restart recognition if it's a no-speech error and not manually stopped\n                if (!this.isManuallyStopped) {\n                    setTimeout(() => {\n                        console.log(\"Restarting speech recognition after no-speech error...\");\n                        this.startRecognitionProcess(onRecognizedCallback); // Restart after no-speech error\n                    }, 1000); // Add a small delay before restarting\n                }\n            } else {\n                console.error(\"Recognition error:\", event.error);\n                this.stopRecognition(); // Stop recognition on other errors\n            }\n        };\n\n        this.recognition.onend = () => {\n            console.log(\"Speech recognition ended.\");\n            this.isRecognizing = false; // Reset the flag when recognition ends\n            if (!this.isManuallyStopped) {\n                console.log(\"Restarting speech recognition...\");\n                this.startRecognitionProcess(onRecognizedCallback); // Automatically restart recognition if not manually stopped\n            }\n        };\n\n        this.recognition.start(); // Start recognition\n        console.log(\"Speech recognition started.\");\n    }\n\n    // Stop recognition if it is running\n    stopRecognition() {\n        if (this.isRecognizing) {\n            this.isManuallyStopped = true; // Set the manual stop flag\n            this.recognition.stop();\n            console.log(\"Speech recognition stopped manually.\");\n            this.isRecognizing = false;\n        }\n    }\n}","import { Box, Icon, Text } from '@chakra-ui/react';\nimport { FaCircle } from 'react-icons/fa';\n\nconst TrafficLightIndicator = ({ status }) => {\n  const colors = {\n    talking: 'red.500',\n    listening: 'green.500',\n    idle: 'yellow.500',\n  };\n\n  return (\n    <Box\n      position=\"fixed\"\n      zIndex=\"1000\"\n      bottom=\"20px\"\n      left=\"20px\"\n      bg=\"gray.700\"\n      borderRadius=\"md\"\n      p={4}\n      boxShadow=\"xl\"\n      userSelect=\"none\"\n    >\n      <Box display=\"flex\" flexDirection=\"column\" alignItems=\"center\">\n        <Icon as={FaCircle} color={colors[status]} boxSize={6} mb={2} />\n        <Text color=\"white\" fontWeight=\"bold\">\n          {status === 'talking' && 'Talking'}\n          {status === 'listening' && 'Listening'}\n          {status === 'idle' && 'Idle'}\n        </Text>\n      </Box>\n    </Box>\n  );\n};\n\nexport default TrafficLightIndicator;","import { useState, useCallback, useRef } from 'react';\nimport ConversationManager from './../VISOS/cognition/ConversationManager'; \n\nconst useConvo = (audioToText, voiceManager, conversationManager, textToSpeakGen, transcribedTextGen) => {\n    const [conversationState, setConversationState] = useState({\n        status: 'idle',\n        transcribedText: null,\n        speakingText: null,\n    });\n\n    // Invoke the generator functions to create iterators\n    const transcribedTextIterator = useRef(transcribedTextGen()).current;\n    const textToSpeakIterator = useRef(textToSpeakGen()).current;\n\n    const handleSpeakingText = useCallback(async (text) => {\n        if (!text || text.trim() === '') {\n            setConversationState((prev) => ({\n                ...prev,\n                status: 'listening',\n                speakingText: null,\n            }));\n            return;\n        }\n\n        setConversationState((prev) => ({\n            ...prev,\n            status: 'talking',\n            speakingText: text,\n        }));\n\n        await voiceManager.enqueueText(text);\n\n        setConversationState((prev) => ({\n            ...prev,\n            status: 'listening',\n            speakingText: null,\n        }));\n        startListening();\n    }, [voiceManager]);\n\n    const processTranscribedText = useCallback((transcribedText) => {\n        setConversationState((prev) => ({\n            ...prev,\n            status: 'thinking',\n            transcribedText,\n        }));\n\n        // Get the next feedback from the transcribedTextGenerator\n        const { value: feedback } = transcribedTextIterator.next(transcribedText);\n\n        if (feedback) {\n            handleSpeakingText(feedback).then(() => {\n                const { value: nextQuestion } = textToSpeakIterator.next();  // Iterate the generator\n                if (nextQuestion) {\n                    handleSpeakingText(nextQuestion);\n                } else {\n                    setConversationState((prev) => ({\n                        ...prev,\n                        status: 'idle',\n                        speakingText: null,\n                    }));\n                }\n            });\n        }\n    }, [handleSpeakingText, transcribedTextIterator, textToSpeakIterator]);\n\n    const startListening = useCallback(() => {\n        setConversationState((prev) => ({ ...prev, status: 'listening' }));\n\n        audioToText.startContinuousRecognition((transcribedText) => {\n            processTranscribedText(transcribedText);\n        });\n    }, [audioToText, processTranscribedText]);\n\n    const startConversation = useCallback(async () => {\n        const { value: firstQuestion } = textToSpeakIterator.next();\n\n        setConversationState((prev) => ({\n            ...prev,\n            status: 'talking',\n            speakingText: firstQuestion,\n        }));\n\n        await voiceManager.enqueueText(firstQuestion);\n\n        setConversationState((prev) => ({ ...prev, status: 'listening', speakingText: null }));\n        startListening();\n    }, [voiceManager, startListening, textToSpeakIterator]);\n\n    const stopConversation = useCallback(() => {\n        audioToText.stopRecognition();\n        voiceManager.stopSpeech();\n        setConversationState({ status: 'idle', transcribedText: null, speakingText: null });\n    }, [audioToText, voiceManager]);\n\n    return { conversationState, startConversation, stopConversation };\n};\n\nexport default useConvo;","class TextToGptReconciler {\n    constructor(apiKey) {\n      if (!apiKey || typeof apiKey !== 'string') {\n        throw new Error('A valid OpenAI API key must be provided.');\n      }\n      this.apiKey = apiKey;\n      this.apiUrl = 'https://api.openai.com/v1/chat/completions';\n    }\n  \n    /**\n     * Processes the provided text by sending it to the OpenAI API.\n     * @param {string} text - The input text to process.\n     * @param {string} instruction - Instruction or system prompt for the AI.\n     * @returns {Promise<string>} - The GPT response.\n     */\n    async processText(text, instruction = 'Answer in a professional manner:') {\n      const payload = {\n        model: 'gpt-3.5-turbo',  // You can switch this model based on your requirement\n        messages: [\n          { role: 'system', content: instruction },\n          { role: 'user', content: text }\n        ],\n        max_tokens: 20,  // Adjust as necessary\n        temperature: 0.88\n      };\n  \n      try {\n        const response = await fetch(this.apiUrl, {\n          method: 'POST',\n          headers: {\n            'Authorization': `Bearer ${this.apiKey}`,\n            'Content-Type': 'application/json'\n          },\n          body: JSON.stringify(payload)\n        });\n  \n        if (!response.ok) {\n          throw new Error(`API request failed with status: ${response.status}`);\n        }\n  \n        const data = await response.json();\n        const gptResponse = data.choices?.[0]?.message?.content?.trim();\n  \n        if (!gptResponse) {\n          throw new Error('Failed to get a valid response from the API');\n        }\n  \n        return gptResponse;\n      } catch (error) {\n        console.error('Error processing text with GPT:', error);\n        throw error;\n      }\n    }\n  }\n  \n  export default TextToGptReconciler;","import ReactDOMClient from 'react-dom/client';\nimport React, { useEffect } from 'react';\nimport { useToast } from '@chakra-ui/react';\nimport AudioToText from './../VISOS/perception/audio/AudioToText';\nimport VoiceManager from './../VISOS/action/verbalizers/VoiceManager';\nimport TrafficLightIndicator from '../components/TrafficLightIndicator';\nimport useConvo from './../hooks/useConvo';\nimport TextToGptReconciler from './../VISOS/cognition/TextToGptReconciler';\n\nlet voiceManager;\nlet audioToText;\nlet gptReconciler;\nlet root = null;\n\nconst initializeModules = async (animationManager, appSettings) => {\n    const { apiKey, preferredVoice } = appSettings;\n\n    audioToText = new AudioToText('webspeech');\n    voiceManager = VoiceManager.getInstance(animationManager);\n    gptReconciler = new TextToGptReconciler(apiKey);\n\n    await voiceManager.findAndSetVoice(preferredVoice || 'Bubbles');\n};\n\nexport const start = async (animationManager, appSettings, containerRef) => {\n    if (!containerRef || !containerRef.current) {\n        console.error('Invalid container reference');\n        return;\n    }\n\n    await initializeModules(animationManager, appSettings);\n\n    const ChatApp = () => {\n        const { conversationState, startConversation, stopConversation } = useConvo(audioToText, voiceManager, gptReconciler);\n        const toast = useToast();\n\n        useEffect(() => {\n            startConversation();\n            return () => stopConversation();\n        }, [startConversation, stopConversation]);\n\n        useEffect(() => {\n            if (conversationState.status === 'thinking') {\n                toast({\n                    title: 'Processing...',\n                    description: 'Processing your response...',\n                    status: 'info',\n                    duration: 2000,\n                });\n            } else if (conversationState.status === 'talking') {\n                toast({\n                    title: 'Response',\n                    description: conversationState.gptResponse,\n                    status: 'success',\n                    duration: 4000,\n                });\n            }\n        }, [conversationState.status, conversationState.gptResponse, toast]);\n\n        return <TrafficLightIndicator status={conversationState.status} />;\n    };\n\n    if (!root) {\n        root = ReactDOMClient.createRoot(containerRef.current);\n    }\n\n    root.render(<ChatApp />);\n};\n\nexport const stop = () => {\n    if (audioToText) {\n        audioToText.stopRecognition();\n    }\n    if (voiceManager) {\n        voiceManager.stopSpeech();\n    }\n    if (root) {\n        root.unmount();\n        root = null;\n    }\n};"],"names":["AudioToText","constructor","recognitionType","arguments","length","undefined","this","recognition","isRecognizing","isManuallyStopped","initRecognition","window","SpeechRecognition","webkitSpeechRecognition","interimResults","continuous","lang","console","error","startContinuousRecognition","onRecognizedCallback","onend","startRecognitionProcess","stopRecognition","warn","finalTranscript","onresult","event","results","i","resultIndex","isFinal","push","transcript","trim","join","onerror","alert","setTimeout","log","start","stop","_ref","status","_jsx","Box","position","zIndex","bottom","left","bg","borderRadius","p","boxShadow","userSelect","children","_jsxs","display","flexDirection","alignItems","Icon","as","FaCircle","color","talking","listening","idle","boxSize","mb","Text","fontWeight","useConvo","audioToText","voiceManager","conversationManager","textToSpeakGen","transcribedTextGen","conversationState","setConversationState","useState","transcribedText","speakingText","transcribedTextIterator","useRef","current","textToSpeakIterator","handleSpeakingText","useCallback","async","text","prev","enqueueText","startListening","processTranscribedText","value","feedback","next","then","nextQuestion","startConversation","firstQuestion","stopConversation","stopSpeech","apiKey","Error","apiUrl","processText","payload","model","messages","role","content","max_tokens","temperature","_data$choices","_data$choices$","_data$choices$$messag","_data$choices$$messag2","response","fetch","method","headers","concat","body","JSON","stringify","ok","gptResponse","json","choices","message","gptReconciler","root","animationManager","appSettings","containerRef","preferredVoice","VoiceManager","getInstance","TextToGptReconciler","findAndSetVoice","initializeModules","ChatApp","toast","useToast","useEffect","title","description","duration","TrafficLightIndicator","ReactDOMClient","render","unmount"],"sourceRoot":""}