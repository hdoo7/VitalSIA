{"version":3,"file":"static/js/631.596b52f7.chunk.js","mappings":"uHAAe,MAAMA,EACjBC,WAAAA,GAA2E,IAA/DC,EAAUC,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,EAAGG,EAAWH,UAAAC,OAAA,EAAAD,UAAA,QAAAE,EAAEE,EAAYJ,UAAAC,OAAA,EAAAD,UAAA,QAAAE,EAAEG,EAAaL,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,EACnEM,KAAKP,WAAaA,EAClBO,KAAKH,YAAcA,EACnBG,KAAKF,aAAeA,EACpBE,KAAKD,cAAgBA,EACrBC,KAAKC,aAAc,CACvB,CAGAC,cAAAA,GACI,OAAO,IAAIC,SAAQ,CAACC,EAASC,KACzBC,QAAQC,IAAI,iCACZP,KAAKC,aAAc,EAEnBD,KAAKH,YAAYW,4BAA4BC,IACrCT,KAAKC,aACLG,EAAQK,EACZ,IAIJT,KAAKH,YAAYa,QAAWC,IACxBL,QAAQK,MAAM,mCAAoCA,GAClDN,EAAOM,EAAM,CAGhB,GAET,CAGAC,aAAAA,GACIN,QAAQC,IAAI,yBACZP,KAAKC,aAAc,EACnBD,KAAKH,YAAYgB,iBACrB,CAGAC,WAAAA,CAAYC,GACR,OAAO,IAAIZ,SAAQ,CAACC,EAASC,KACzBC,QAAQC,IAAI,iBAAkBQ,GAC9Bf,KAAKY,gBACLZ,KAAKF,aAAagB,YAAYC,GAAMC,MAAK,KACrCV,QAAQC,IAAI,qBACZH,IACAa,YAAW,KACPjB,KAAKkB,iBAAiB,GACvBlB,KAAKP,WAAW,IACpB0B,OAAMR,IACLL,QAAQK,MAAM,uBAAwBA,GACtCN,EAAOM,EAAM,GACf,GAEV,CAGAO,eAAAA,GACIZ,QAAQC,IAAI,yBACZP,KAAKC,aAAc,EACnBD,KAAKE,iBAAiBc,MAAMD,IACpBf,KAAKoB,mBAAmBL,IACxBf,KAAKqB,mBAAmBN,EAC5B,IACDI,OAAOR,IACNL,QAAQK,MAAM,mCAAoCA,EAAM,GAEhE,CAGAS,kBAAAA,CAAmBL,GAEf,OADkBA,EAAKO,MAAM,KAAK3B,QACdK,KAAKD,aAC7B,CAGAsB,kBAAAA,CAAmBE,GACfjB,QAAQC,IAAI,+BAADiB,OAAgCD,IAC3CvB,KAAKF,aAAa2B,aAClBzB,KAAKkB,iBACT,CAGAQ,4BAAAA,CAA6BC,GACzB,OAAO,IAAIxB,SAASC,IAChBa,YAAW,KACPU,EAAU,aACV3B,KAAKE,iBAAiBc,MAAMD,IACxBX,EAAQW,EAAK,GACf,GACHf,KAAKP,WAAW,GAE3B,E,gDC5FW,MAAMmC,EACjBpC,WAAAA,GAA4C,IAAhCqC,EAAenC,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,YAC1BM,KAAK8B,YAAc,KACnB9B,KAAK+B,eAAgB,EACrB/B,KAAKgC,mBAAoB,EACzBhC,KAAKiC,gBAAgBJ,EACzB,CAEAI,eAAAA,CAAgBJ,GACY,cAApBA,GACA7B,KAAK8B,YAAc,IAAKI,OAAOC,mBAAqBD,OAAOE,yBAC3DpC,KAAK8B,YAAYO,gBAAiB,EAClCrC,KAAK8B,YAAYQ,YAAa,EAC9BtC,KAAK8B,YAAYS,KAAO,SAExBjC,QAAQK,MAAM,+BAEtB,CAGAH,0BAAAA,CAA2BgC,GACnBxC,KAAK+B,eAEL/B,KAAK8B,YAAYW,MAAQ,KACrBzC,KAAK+B,eAAgB,EACrB/B,KAAK0C,wBAAwBF,EAAqB,EAEtDxC,KAAKa,oBAGLb,KAAKgC,mBAAoB,EACzBhC,KAAK0C,wBAAwBF,GAErC,CAGAE,uBAAAA,CAAwBF,GACpB,GAAIxC,KAAK+B,cAEL,YADAzB,QAAQqC,KAAK,mDAIjB3C,KAAK+B,eAAgB,EACrB,MAAMa,EAAkB,GAExB5C,KAAK8B,YAAYe,SAAYC,IACzB,MAAMC,EAAUD,EAAMC,QACtB,IAAK,IAAIC,EAAIF,EAAMG,YAAaD,EAAID,EAAQpD,OAAQqD,IAC5CD,EAAQC,GAAGE,SACXN,EAAgBO,KAAKJ,EAAQC,GAAG,GAAGI,WAAWC,QAGtDb,EAAqBI,EAAgBU,KAAK,KAAK,EAInDtD,KAAK8B,YAAYpB,QAAWoC,IACJ,gBAAhBA,EAAMnC,OACNL,QAAQK,MAAM,yDACd4C,MAAM,yEACNvD,KAAKa,mBACkB,cAAhBiC,EAAMnC,OACbL,QAAQqC,KAAK,0CAER3C,KAAKgC,mBACNf,YAAW,KACPX,QAAQC,IAAI,0DACZP,KAAK0C,wBAAwBF,EAAqB,GACnD,OAGPlC,QAAQK,MAAM,qBAAsBmC,EAAMnC,OAC1CX,KAAKa,kBACT,EAGJb,KAAK8B,YAAYW,MAAQ,KACrBnC,QAAQC,IAAI,6BACZP,KAAK+B,eAAgB,EAChB/B,KAAKgC,oBACN1B,QAAQC,IAAI,oCACZP,KAAK0C,wBAAwBF,GACjC,EAGJxC,KAAK8B,YAAY0B,QACjBlD,QAAQC,IAAI,8BAChB,CAGAM,eAAAA,GACQb,KAAK+B,gBACL/B,KAAKgC,mBAAoB,EACzBhC,KAAK8B,YAAY2B,OACjBnD,QAAQC,IAAI,wCACZP,KAAK+B,eAAgB,EAE7B,E,0EC/FW,MAAM2B,EACjBlE,WAAAA,GACIQ,KAAK2D,aAAe,KACpB3D,KAAK4D,cAAe,EACpB5D,KAAK6D,WAAa,IACtB,CAGAC,SAAAA,CAAUH,GACN3D,KAAK2D,aAAeA,CACxB,CAGA,gBAAMI,GACF,MAAMC,EAAYC,oBAClB,UACUC,EAAAA,GAAaC,iBAAiBC,YAAYJ,SAC1CE,EAAAA,GAAaG,kBAAkBD,YAAYJ,SAC3CE,EAAAA,GAAaI,kBAAkBF,YAAYJ,SAC3CE,EAAAA,GAAaK,mBAAmBH,YAAYJ,GAClD1D,QAAQC,IAAI,6BAChB,CAAE,MAAOI,GAEL,MADAL,QAAQK,MAAM,wBAAyBA,GACjC,IAAI6D,MAAM,iCACpB,CACJ,CAGA,sBAAMC,GACF,IACI,IAAKzE,KAAK2D,aAAc,MAAM,IAAIa,MAAM,wCAGlCxE,KAAK+D,aAEX,MAAMW,QAAeC,UAAUC,aAAaC,aAAa,CAAEC,OAAO,IAClE9E,KAAK2D,aAAaoB,UAAYL,CAClC,CAAE,MAAO/D,GACLL,QAAQK,MAAM,0BAA2BA,EAC7C,CACJ,CAGAqE,wBAAAA,CAAyBC,GACrB,GAAIjF,KAAK4D,aAEL,YADAtD,QAAQqC,KAAK,yCAIjB3C,KAAK4D,cAAe,EAgBpB5D,KAAK6D,WAAaqB,aAdKC,UACnB,IAAKnF,KAAK2D,aAAc,OAExB,MAAMyB,QAAmBlB,EAAAA,EACLlE,KAAK2D,aAAc,IAAIO,EAAAA,IACtCmB,oBACAC,sBAEL,GAAIF,EAAWzF,OAAS,EAAG,CACvB,MAAM4F,EAAWH,EAAW,GAAGI,YAC/BP,EAA0BM,EAC9B,IAG0C,IAClD,CAGAE,aAAAA,GACQzF,KAAK4D,eACL8B,cAAc1F,KAAK6D,YACnB7D,KAAK4D,cAAe,EAE5B,CAGA+B,eAAAA,GACI,GAAI3F,KAAK2D,cAAgB3D,KAAK2D,aAAaoB,UAAW,CACnC/E,KAAK2D,aAAaoB,UAC1Ba,YAAYC,SAAQC,GAASA,EAAMrC,SAC1CzD,KAAK2D,aAAaoB,UAAY,IAClC,CACJ,EClFJ,MAoEA,EApEegB,KACX,MAAOC,EAAcC,IAAmBC,EAAAA,EAAAA,UAAS,CAC7CC,gBAAiB,KACjBC,mBAAoB,CAChBC,IAAK,EACLC,MAAO,EACPC,QAAS,EACTC,KAAM,EACNC,QAAS,EACTC,SAAU,GAEdC,OAAQ,SAGNC,GAAiBC,EAAAA,EAAAA,QAAO,IAAInD,GAAkBoD,QAC9CC,GAAkBF,EAAAA,EAAAA,QAAO,MA6C/B,MAAO,CACHb,eACAgB,mBA5CsBC,EAAAA,EAAAA,cAAY9B,UAClC,GAAK4B,EAAgBD,QAKrB,IAEIF,EAAe9C,UAAUiD,EAAgBD,eACnCF,EAAenC,mBACrBwB,GAAiBiB,IAAI,IAAWA,EAAMP,OAAQ,gBAG9CC,EAAe5B,0BAA0BO,IACrC,MAAMa,EAAqB,CACvBC,IAAKd,EAAS4B,OAAS,EACvBb,MAAOf,EAAS6B,OAAS,EACzBb,QAAShB,EAAS8B,WAAa,EAC/Bb,KAAMjB,EAAS+B,SAAW,EAC1Bb,QAASlB,EAASgC,KAAO,EACzBb,SAAUnB,EAASiC,WAAa,GAE9BC,EAAaC,OAAOC,KAAKvB,GAAoBwB,QAAO,CAACC,EAAGC,IAAM1B,EAAmByB,GAAKzB,EAAmB0B,GAAKD,EAAIC,IACxH7B,EAAgB,CACZE,gBAAiBsB,EACjBrB,qBACAO,OAAQ,aACV,GAEV,CAAE,MAAOhG,GACLL,QAAQK,MAAM,oCAAqCA,EACvD,MA7BIL,QAAQK,MAAM,8BA6BlB,GACD,CAACiG,IAaAmB,kBAVqBd,EAAAA,EAAAA,cAAY,KACjCL,EAAenB,gBACfmB,EAAejB,kBACfM,EAAgB,CAAEE,gBAAiB,KAAMQ,OAAQ,UAAWP,mBAAoB,CAAC,GAAI,GACtF,CAACQ,IAOAG,kBACH,E,0DChEL,MA6EA,EA7E0BiB,IAAwC,IAAvC,gBAAEjB,EAAe,aAAEf,GAAcgC,EAC1D,MAAOC,EAAUC,IAAehC,EAAAA,EAAAA,UAAS,CAAEiC,EAAG,EAAGC,EAAG,KAC7CC,EAAUC,IAAepC,EAAAA,EAAAA,WAAS,IAClCqC,EAAWC,IAAgBtC,EAAAA,EAAAA,WAAS,GA+B3C,OACEuC,EAAAA,EAAAA,MAACC,EAAAA,GAAG,CACFT,SAAS,QACTU,MAAOJ,EAAY,QAAU,QAC7BK,OAAQL,EAAY,OAAS,QAC7BM,OAAQN,EAAY,UAAY,OAChCO,YAAcP,EAA8B,KAlCvBzF,IACvBwF,GAAY,EAAK,EAkCfS,YAAcR,EAA8B,KA/BvBzF,IACvB,GAAIuF,EAAU,CACZ,MAAMF,EAAIrF,EAAMkG,QAAUlG,EAAMmG,OAAOC,YAAc,EAC/Cd,EAAItF,EAAMqG,QAAUrG,EAAMmG,OAAOG,aAAe,EACtDlB,EAAY,CAAEC,IAAGC,KACnB,GA2BEiB,UAAYd,EAA4B,KAxBtBe,KACpBhB,GAAY,GAGZ,MAAMiB,EAAcrH,OAAOsH,WACrBC,EAAOxB,EAASE,EAAIoB,EAAc,EAAI,GAAKA,EAAczG,MAAMmG,OAAOC,YAAc,GACpFQ,EAAOxH,OAAOyH,YAAc7G,MAAMmG,OAAOG,aAAe,GAE9DlB,EAAY,CAAEC,EAAGsB,EAAMrB,EAAGsB,GAAO,EAiB/BE,MAAO,CACLC,KAAK,GAADrI,OAAKyG,EAASE,EAAC,MACnB2B,IAAI,GAADtI,OAAKyG,EAASG,EAAC,MAClB2B,OAAQ,KAEVC,OAAO,iBACPC,aAAa,OACbC,SAAS,SACTC,UAAU,KACVC,GAAG,WAAUC,SAAA,EAEbC,EAAAA,EAAAA,KAAC5B,EAAAA,GAAG,CAACT,SAAS,WAAW6B,IAAI,IAAIS,MAAM,IAAGF,UACxCC,EAAAA,EAAAA,KAACE,EAAAA,EAAU,CACTC,KAAMlC,GAAY+B,EAAAA,EAAAA,KAACI,EAAAA,EAAa,KAAMJ,EAAAA,EAAAA,KAACK,EAAAA,EAAe,IACtDC,KAAK,KACLC,QA5BeC,KACrBtC,GAAcD,EAAU,EA4BlB,aAAW,uBAIbA,IACAE,EAAAA,EAAAA,MAAAsC,EAAAA,SAAA,CAAAV,SAAA,EACEC,EAAAA,EAAAA,KAAA,SAAOU,IAAKjE,EAAiBkE,UAAQ,EAACC,OAAK,EAACtB,MAAO,CAAEjB,MAAO,OAAQC,OAAQ,WAC5E0B,EAAAA,EAAAA,KAAC5B,EAAAA,GAAG,CAACT,SAAS,WAAWkD,OAAO,IAAItB,KAAK,IAAIuB,EAAE,IAAIC,MAAM,QAAQjB,GAAG,qBAAoBC,SACrFrE,EAAaG,kBACZsC,EAAAA,EAAAA,MAAA,OAAA4B,SAAA,CAAK,qBAAmBrE,EAAaG,0BAKzC,E,uBChEVmF,EAAAA,GAAQC,SAASC,EAAAA,GAAmBC,EAAAA,GAAcC,EAAAA,GAAaC,EAAAA,GAAQC,EAAAA,GAASC,EAAAA,IAEhF,MAmCA,EAnC0B7D,IAA6B,IAA5B,mBAAE5B,GAAoB4B,EAC7C,MAAM8D,EAAO,CACTC,OAAQ,CAAC,MAAO,QAAS,UAAW,OAAQ,UAAW,YACvDC,SAAU,CACN,CACIC,MAAO,sBACPH,KAAM,CACF1F,EAAmBC,IACnBD,EAAmBE,MACnBF,EAAmBG,QACnBH,EAAmBI,KACnBJ,EAAmBK,QACnBL,EAAmBM,UAEvBwF,gBAAiB,0BACjBC,YAAa,wBACbC,YAAa,KAYzB,OACI9B,EAAAA,EAAAA,KAAC5B,EAAAA,GAAG,CAACC,MAAM,QAAQC,OAAO,QAAQwC,EAAG,EAAEf,UACnCC,EAAAA,EAAAA,KAAC+B,EAAAA,GAAK,CAACP,KAAMA,EAAMQ,QATX,CACZC,MAAO,CACHC,MAAO,CAAEC,aAAa,EAAMC,IAAK,GACjCC,WAAY,CAAEC,SAAS,QAOrB,E,eCxCd,MA4BA,EA5ByB5E,IAA+B,IAA9B,qBAAE6E,GAAsB7E,EAC9C,MAAM,aAAEhC,EAAY,kBAAEgB,EAAiB,iBAAEe,EAAgB,gBAAEhB,GAAoBhB,IAkB/E,OAfA+G,EAAAA,EAAAA,YAAU,KACN9F,IAEO,KACHe,GAAkB,IAEvB,CAACf,EAAmBe,KAGvB+E,EAAAA,EAAAA,YAAU,KACFD,GACAA,EAAqB7G,EACzB,GACD,CAACA,EAAc6G,KAGdpE,EAAAA,EAAAA,MAAA,OAAA4B,SAAA,EACIC,EAAAA,EAAAA,KAACyC,EAAiB,CAAChG,gBAAiBA,EAAiBf,aAAcA,KACnEsE,EAAAA,EAAAA,KAAC0C,EAAiB,CAAC5G,mBAAoBJ,EAAaI,sBACpDkE,EAAAA,EAAAA,KAAC2C,EAAAA,EAAQ,CAAClM,KAAMiF,EAAaG,iBAAmB,OAC9C,C,2GC3Bd,MA+BA,EA/B8B6B,IAAiB,IAAhB,OAAErB,GAAQqB,EAOvC,OACEsC,EAAAA,EAAAA,KAAC5B,EAAAA,GAAG,CACFT,SAAS,QACT8B,OAAO,OACPoB,OAAO,OACPtB,KAAK,OACLO,GAAG,WACHH,aAAa,KACbmB,EAAG,EACHjB,UAAU,KACV+C,WAAW,OAAM7C,UAEjB5B,EAAAA,EAAAA,MAACC,EAAAA,GAAG,CAACkE,QAAQ,OAAOO,cAAc,SAASC,WAAW,SAAQ/C,SAAA,EAC5DC,EAAAA,EAAAA,KAAC+C,EAAAA,EAAI,CAACC,GAAIC,EAAAA,IAAUlC,MAnBX,CACbmC,QAAS,UACTC,UAAW,YACXC,KAAM,cAgBgC/G,GAASgH,QAAS,EAAGC,GAAI,KAC3DnF,EAAAA,EAAAA,MAACoF,EAAAA,EAAI,CAACxC,MAAM,QAAQyC,WAAW,OAAMzD,SAAA,CACvB,YAAX1D,GAAwB,UACb,cAAXA,GAA0B,YACf,SAAXA,GAAqB,cAGtB,C,+DC3BV,MA+FA,EA/FiBoH,CAAClO,EAAaC,EAAckO,EAAqBC,EAAgBC,KAC9E,MAAOC,EAAmBC,IAAwBlI,EAAAA,EAAAA,UAAS,CACvDS,OAAQ,OACRlG,gBAAiB,KACjB4N,aAAc,OAIZC,GAA0BzH,EAAAA,EAAAA,QAAOqH,KAAsBpH,QACvDyH,GAAsB1H,EAAAA,EAAAA,QAAOoH,KAAkBnH,QAE/C0H,GAAqBvH,EAAAA,EAAAA,cAAY9B,UAC9BpE,GAAwB,KAAhBA,EAAKsC,QASlB+K,GAAsBlH,IAAI,IACnBA,EACHP,OAAQ,UACR0H,aAActN,YAGZjB,EAAagB,YAAYC,GAE/BqN,GAAsBlH,IAAI,IACnBA,EACHP,OAAQ,YACR0H,aAAc,SAElBnO,KArBIkO,GAAsBlH,IAAI,IACnBA,EACHP,OAAQ,YACR0H,aAAc,QAkBN,GACjB,CAACvO,IAEE2O,GAAyBxH,EAAAA,EAAAA,cAAaxG,IACxC2N,GAAsBlH,IAAI,IACnBA,EACHP,OAAQ,WACRlG,sBAIJ,MAAQiO,MAAOC,GAAaL,EAAwBM,KAAKnO,GAErDkO,GACAH,EAAmBG,GAAU3N,MAAK,KAC9B,MAAQ0N,MAAOG,GAAiBN,EAAoBK,OAChDC,EACAL,EAAmBK,GAEnBT,GAAsBlH,IAAI,IACnBA,EACHP,OAAQ,OACR0H,aAAc,QAEtB,GAER,GACD,CAACG,EAAoBF,EAAyBC,IAE3CrO,GAAiB+G,EAAAA,EAAAA,cAAY,KAC/BmH,GAAsBlH,IAAI,IAAWA,EAAMP,OAAQ,gBAEnD9G,EAAYW,4BAA4BC,IACpCgO,EAAuBhO,EAAgB,GACzC,GACH,CAACZ,EAAa4O,IAuBjB,MAAO,CAAEN,oBAAmBW,mBArBF7H,EAAAA,EAAAA,cAAY9B,UAClC,MAAQuJ,MAAOK,GAAkBR,EAAoBK,OAErDR,GAAsBlH,IAAI,IACnBA,EACHP,OAAQ,UACR0H,aAAcU,YAGZjP,EAAagB,YAAYiO,GAE/BX,GAAsBlH,IAAI,IAAWA,EAAMP,OAAQ,YAAa0H,aAAc,SAC9EnO,GAAgB,GACjB,CAACJ,EAAcI,EAAgBqO,IAQaS,kBANtB/H,EAAAA,EAAAA,cAAY,KACjCpH,EAAYgB,kBACZf,EAAa2B,aACb2M,EAAqB,CAAEzH,OAAQ,OAAQlG,gBAAiB,KAAM4N,aAAc,MAAO,GACpF,CAACxO,EAAaC,IAEgD,C,+DC5FrE,MAyDA,EAzDqBmP,CAACC,EAAkBlJ,MAEpC8G,EAAAA,EAAAA,YAAU,KACN,GAAI9G,GAAgBA,EAAaG,gBAAiB,CAC9C,MAAM,gBAAEA,EAAe,mBAAEC,GAAuBJ,EAEhD1F,QAAQC,IAAI,oBAAqB4F,GACjC7F,QAAQC,IAAI,qBAAsB6F,EAAmBD,IAGrDgJ,EAAqBhJ,EAAiBC,EAAmBD,GAAkB+I,EAC/E,IACD,CAAClJ,EAAckJ,IAGlB,MAAMC,EAAuBA,CAACC,EAASC,EAAWH,KAC9C5O,QAAQC,IAAI,aAADiB,OAAc4N,EAAO,qBAAA5N,OAAoB6N,IAGpD,IAAIC,EAAoB,QAAZF,GAAiC,aAAZA,EAAyB,EAAI,IAE9D,OAAQA,GACJ,IAAK,OCzBV,SAAmBF,GAAoD,IAAlCG,EAAS3P,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,IAAK6P,EAAQ7P,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,IAEpEwP,EAAiBM,eAAe,KAAMH,EAAWE,EAAU,GAC3DL,EAAiBM,eAAe,IAAiB,GAAZH,EAAiBE,EAAU,EACpE,CDsBgBE,CAAUP,EAA8B,IAAZG,EAAiB,KAC7C,MACJ,IAAK,UACDpO,YAAW,MCvBpB,SAAiBiO,GAAoD,IAAlCG,EAAS3P,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,IAAK6P,EAAQ7P,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,IAElEwP,EAAiBM,eAAe,IAAKH,EAAWE,EAAU,GAC1DL,EAAiBM,eAAe,KAAkB,GAAZH,EAAiBE,EAAU,EACrE,CDoBoBG,CAAQR,EAA8B,IAAZG,EAAiB,IAAI,GAChDC,GACH,MACJ,IAAK,QACDrO,YAAW,MCtBpB,SAAmBiO,GAAoD,IAAlCG,EAAS3P,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,IAAK6P,EAAQ7P,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,IAEpEwP,EAAiBM,eAAe,IAAKH,EAAWE,EAAU,GAC1DL,EAAiBM,eAAe,IAAKH,EAAWE,EAAU,EAC9D,CDmBoBI,CAAUT,EAA8B,IAAZG,EAAiB,IAAI,GAClDC,GACH,MACJ,IAAK,YCdV,SAAsBJ,GAAoD,IAAlCG,EAAS3P,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,IAAK6P,EAAQ7P,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,IAEvEwP,EAAiBM,eAAe,IAAKH,EAAWE,EAAU,GAC1DL,EAAiBM,eAAe,IAAiB,GAAZH,EAAiBE,EAAU,EACpE,CDWgBK,CAAaV,EAA8B,IAAZG,EAAiB,KAChD,MACJ,IAAK,OACDpO,YAAW,MCxBpB,SAAkBiO,GAAoD,IAAlCG,EAAS3P,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,IAAK6P,EAAQ7P,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,IAEnEwP,EAAiBM,eAAe,IAAKH,EAAWE,EAAU,GAC1DL,EAAiBM,eAAe,KAAkB,GAAZH,EAAiBE,EAAU,EACrE,CDqBoBM,CAASX,EAA8B,IAAZG,EAAiB,IAAI,GACjDC,GACH,MACJ,IAAK,UACDrO,YAAW,MCjBpB,SAAqBiO,GAAoD,IAAlCG,EAAS3P,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,IAAK6P,EAAQ7P,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,IAEtEwP,EAAiBM,eAAe,IAAKH,EAAWE,EAAU,GAC1DL,EAAiBM,eAAe,KAAkB,GAAZH,EAAiBE,EAAU,EACrE,CDcoBO,CAAYZ,EAA8B,IAAZG,EAAiB,IAAI,GACpDC,GACH,MACJ,QACIhP,QAAQC,IAAI,6BAEpB,EAGJ,OAAO,IAAI,C","sources":["VISOS/cognition/ConversationManager.js","VISOS/perception/audio/AudioToText.js","VISOS/perception/video/VideoToEmotion.js","hooks/useEmo.js","components/DraggableVideoBox.jsx","components/EmotionRadarChart.jsx","components/EmotionDetection.jsx","components/TrafficLightIndicator.jsx","hooks/useConvo.js","hooks/useMirroring.js","VISOS/action/visualizers/basicEmotions.js"],"sourcesContent":["export default class ConversationManager {\n    constructor(bufferTime = 0, audioToText, voiceManager, wordThreshold = 5) {\n        this.bufferTime = bufferTime;\n        this.audioToText = audioToText;\n        this.voiceManager = voiceManager;  // Now accepts voiceManager\n        this.wordThreshold = wordThreshold; // Number of words to trigger interruption\n        this.isListening = false;\n    }\n\n    // Start listening and handle incoming transcriptions as a promise\n    startListening() {\n        return new Promise((resolve, reject) => {\n            console.log(\"Starting listening session...\");\n            this.isListening = true;\n\n            this.audioToText.startContinuousRecognition((transcribedText) => {\n                if (this.isListening) {\n                    resolve(transcribedText); // Resolve the promise when text is transcribed\n                }\n            });\n\n            // Handle errors from audioToText if necessary\n            this.audioToText.onerror = (error) => {\n                console.error(\"Error during speech recognition:\", error);\n                reject(error);  // Reject the promise if there's an error\n\n\n            };\n        });\n    }\n\n    // Stop listening\n    stopListening() {\n        console.log(\"Stopping listening...\");\n        this.isListening = false;\n        this.audioToText.stopRecognition();\n    }\n\n    // Enqueue text and handle speaking while controlling listening behavior\n    enqueueText(text) {\n        return new Promise((resolve, reject) => {\n            console.log(\"Speaking text:\", text);\n            this.stopListening();  // Stop listening while speaking\n            this.voiceManager.enqueueText(text).then(() => {\n                console.log(\"Finished speaking\");\n                resolve();  // Return promise once speaking is done\n                setTimeout(() => {\n                    this.resumeListening();  // Resume listening after speaking\n                }, this.bufferTime);\n            }).catch(error => {\n                console.error(\"Error during speech:\", error);\n                reject(error);\n            });\n        });\n    }\n\n    // Resume listening after speaking or other action\n    resumeListening() {\n        console.log(\"Resuming listening...\");\n        this.isListening = true;\n        this.startListening().then((text) => {\n            if (this.detectInterruption(text)) {\n                this.handleInterruption(text);\n            }\n        }).catch((error) => {\n            console.error(\"Error during resuming listening:\", error);\n        });\n    }\n\n    // Detect if the user has spoken more than the word threshold during the agent's speech\n    detectInterruption(text) {\n        const wordCount = text.split(' ').length;\n        return wordCount >= this.wordThreshold;\n    }\n\n    // Handle interruption by stopping the agent's speech and returning to listening\n    handleInterruption(userText) {\n        console.log(`User interruption detected: ${userText}`);\n        this.voiceManager.stopSpeech();  // Stop current speech\n        this.resumeListening();  // Return to listening immediately\n    }\n\n    // Resume listening after a response with a promise-based return\n    resumeListeningAfterResponse(setStatus) {\n        return new Promise((resolve) => {\n            setTimeout(() => {\n                setStatus('listening');  // Update status to listening\n                this.startListening().then((text) => {\n                    resolve(text);  // Resolve with the transcribed text\n                });\n            }, this.bufferTime);\n        });\n    }\n}","export default class AudioToText {\n    constructor(recognitionType = 'webspeech') {\n        this.recognition = null;\n        this.isRecognizing = false;\n        this.isManuallyStopped = false; // New flag to track manual stopping\n        this.initRecognition(recognitionType);\n    }\n\n    initRecognition(recognitionType) {\n        if (recognitionType === 'webspeech') {\n            this.recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();\n            this.recognition.interimResults = false;\n            this.recognition.continuous = true;\n            this.recognition.lang = 'en-US';\n        } else {\n            console.error(\"Unsupported recognition type\");\n        }\n    }\n\n    // Start recognition, but first stop any existing recognition\n    startContinuousRecognition(onRecognizedCallback) {\n        if (this.isRecognizing) {\n            // Recognition is already running, so stop it first\n            this.recognition.onend = () => {\n                this.isRecognizing = false;\n                this.startRecognitionProcess(onRecognizedCallback); // Start the recognition after stopping\n            };\n            this.stopRecognition();\n        } else {\n            // Start recognition directly if it is not already running\n            this.isManuallyStopped = false; // Reset manual stop flag\n            this.startRecognitionProcess(onRecognizedCallback);\n        }\n    }\n\n    // Process for starting recognition\n    startRecognitionProcess(onRecognizedCallback) {\n        if (this.isRecognizing) {\n            console.warn(\"Recognition is already running, skipping start.\");\n            return; // Avoid starting if it's already running\n        }\n\n        this.isRecognizing = true;\n        const finalTranscript = [];\n\n        this.recognition.onresult = (event) => {\n            const results = event.results;\n            for (let i = event.resultIndex; i < results.length; i++) {\n                if (results[i].isFinal) {\n                    finalTranscript.push(results[i][0].transcript.trim());\n                }\n            }\n            onRecognizedCallback(finalTranscript.join(' ')); // Send the transcription result\n        };\n\n        // Handle recognition errors\n        this.recognition.onerror = (event) => {\n            if (event.error === 'not-allowed') {\n                console.error(\"Recognition error: Microphone access was not allowed.\");\n                alert(\"Please allow microphone access to use the speech recognition feature.\");\n                this.stopRecognition(); // Stop recognition on permission error\n            } else if (event.error === 'no-speech') {\n                console.warn(\"Recognition error: No speech detected.\");\n                // Automatically restart recognition if it's a no-speech error and not manually stopped\n                if (!this.isManuallyStopped) {\n                    setTimeout(() => {\n                        console.log(\"Restarting speech recognition after no-speech error...\");\n                        this.startRecognitionProcess(onRecognizedCallback); // Restart after no-speech error\n                    }, 1000); // Add a small delay before restarting\n                }\n            } else {\n                console.error(\"Recognition error:\", event.error);\n                this.stopRecognition(); // Stop recognition on other errors\n            }\n        };\n\n        this.recognition.onend = () => {\n            console.log(\"Speech recognition ended.\");\n            this.isRecognizing = false; // Reset the flag when recognition ends\n            if (!this.isManuallyStopped) {\n                console.log(\"Restarting speech recognition...\");\n                this.startRecognitionProcess(onRecognizedCallback); // Automatically restart recognition if not manually stopped\n            }\n        };\n\n        this.recognition.start(); // Start recognition\n        console.log(\"Speech recognition started.\");\n    }\n\n    // Stop recognition if it is running\n    stopRecognition() {\n        if (this.isRecognizing) {\n            this.isManuallyStopped = true; // Set the manual stop flag\n            this.recognition.stop();\n            console.log(\"Speech recognition stopped manually.\");\n            this.isRecognizing = false;\n        }\n    }\n}","import * as faceapi from 'face-api.js';\n\nexport default class VideoToEmotion {\n    constructor() {\n        this.videoElement = null;\n        this.isProcessing = false;\n        this.intervalId = null;\n    }\n\n    // Initialize the video element for processing\n    initVideo(videoElement) {\n        this.videoElement = videoElement;\n    }\n\n    // Load face-api.js models before detection\n    async loadModels() {\n        const MODEL_URL = process.env.PUBLIC_URL + '/models';\n        try {\n            await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);\n            await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);\n            await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);\n            await faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL);\n            console.log('Models loaded successfully');\n        } catch (error) {\n            console.error('Error loading models:', error);\n            throw new Error('Failed to load face-api models');\n        }\n    }\n\n    // Start the webcam stream and setup face detection\n    async startVideoStream() {\n        try {\n            if (!this.videoElement) throw new Error(\"Video element not initialized.\");\n\n            // Ensure models are loaded before starting the video stream\n            await this.loadModels();\n\n            const stream = await navigator.mediaDevices.getUserMedia({ video: true });\n            this.videoElement.srcObject = stream;  // Attach webcam stream to video element\n        } catch (error) {\n            console.error(\"Error accessing webcam:\", error);\n        }\n    }\n\n    // Start detecting emotions from the video feed\n    startContinuousDetection(onEmotionDetectedCallback) {\n        if (this.isProcessing) {\n            console.warn(\"Emotion detection is already running.\");\n            return;\n        }\n\n        this.isProcessing = true;\n\n        const detectEmotions = async () => {\n            if (!this.videoElement) return;\n\n            const detections = await faceapi\n                .detectAllFaces(this.videoElement, new faceapi.TinyFaceDetectorOptions())\n                .withFaceLandmarks()\n                .withFaceExpressions();\n\n            if (detections.length > 0) {\n                const emotions = detections[0].expressions;\n                onEmotionDetectedCallback(emotions);\n            }\n        };\n\n        this.intervalId = setInterval(detectEmotions, 100); // Detect emotions every 100ms\n    }\n\n    // Stop emotion detection\n    stopDetection() {\n        if (this.isProcessing) {\n            clearInterval(this.intervalId);\n            this.isProcessing = false;\n        }\n    }\n\n    // Stop the video stream when unmounting\n    stopVideoStream() {\n        if (this.videoElement && this.videoElement.srcObject) {\n            const stream = this.videoElement.srcObject;\n            stream.getTracks().forEach(track => track.stop());\n            this.videoElement.srcObject = null;\n        }\n    }\n}","import { useState, useCallback, useRef } from 'react';\nimport VideoToEmotion from '../VISOS/perception/video/VideoToEmotion';  // Import Video Sensor\n\nconst useEmo = () => {\n    const [emotionState, setEmotionState] = useState({\n        detectedEmotion: null,\n        emotionIntensities: {\n            joy: 0,\n            anger: 0,\n            disgust: 0,\n            fear: 0,\n            sadness: 0,\n            surprise: 0\n        },\n        status: 'idle',  // Possible statuses: 'idle', 'detecting', 'stopped'\n    });\n\n    const videoToEmotion = useRef(new VideoToEmotion()).current;  // Video sensor instance\n    const videoElementRef = useRef(null);  // Reference to the video element\n\n    // Function to start emotion detection and video stream\n    const startEmoDetection = useCallback(async () => {\n        if (!videoElementRef.current) {\n            console.error(\"Video element not provided.\");\n            return;\n        }\n\n        try {\n            // Initialize video sensor with video element and start video stream\n            videoToEmotion.initVideo(videoElementRef.current);\n            await videoToEmotion.startVideoStream();\n            setEmotionState((prev) => ({ ...prev, status: 'detecting' }));\n\n            // Start continuous emotion detection\n            videoToEmotion.startContinuousDetection((emotions) => {\n                const emotionIntensities = {\n                    joy: emotions.happy || 0,\n                    anger: emotions.angry || 0,\n                    disgust: emotions.disgusted || 0,\n                    fear: emotions.fearful || 0,\n                    sadness: emotions.sad || 0,\n                    surprise: emotions.surprised || 0\n                };\n                const topEmotion = Object.keys(emotionIntensities).reduce((a, b) => emotionIntensities[a] > emotionIntensities[b] ? a : b);\n                setEmotionState({\n                    detectedEmotion: topEmotion,\n                    emotionIntensities,\n                    status: 'detecting'\n                });\n            });\n        } catch (error) {\n            console.error('Error starting emotion detection:', error);\n        }\n    }, [videoToEmotion]);\n\n    // Function to stop emotion detection and video stream\n    const stopEmoDetection = useCallback(() => {\n        videoToEmotion.stopDetection();  // Stop emotion detection\n        videoToEmotion.stopVideoStream();  // Stop video stream\n        setEmotionState({ detectedEmotion: null, status: 'stopped', emotionIntensities: {} });\n    }, [videoToEmotion]);\n\n    // Return emotion state and control functions\n    return {\n        emotionState,\n        startEmoDetection,\n        stopEmoDetection,\n        videoElementRef,  // Reference to be attached to video element\n    };\n};\n\nexport default useEmo;","import React, { useState, useEffect } from 'react';\nimport { Box, IconButton } from '@chakra-ui/react';\nimport { ChevronUpIcon, ChevronDownIcon } from '@chakra-ui/icons';\n\nconst DraggableVideoBox = ({ videoElementRef, emotionState }) => {\n  const [position, setPosition] = useState({ x: 0, y: 0 });\n  const [dragging, setDragging] = useState(false);\n  const [collapsed, setCollapsed] = useState(false);\n\n  // Handle dragging behavior\n  const handleMouseDown = (event) => {\n    setDragging(true);\n  };\n\n  const handleMouseMove = (event) => {\n    if (dragging) {\n      const x = event.clientX - event.target.offsetWidth / 2;\n      const y = event.clientY - event.target.offsetHeight / 2;\n      setPosition({ x, y });\n    }\n  };\n\n  const handleMouseUp = () => {\n    setDragging(false);\n\n    // Snap to corner logic: bottom-left or bottom-right based on position\n    const windowWidth = window.innerWidth;\n    const newX = position.x < windowWidth / 2 ? 10 : windowWidth - event.target.offsetWidth - 10;\n    const newY = window.innerHeight - event.target.offsetHeight - 10;\n\n    setPosition({ x: newX, y: newY });\n  };\n\n  // Toggle collapse state\n  const toggleCollapse = () => {\n    setCollapsed(!collapsed);\n  };\n\n  return (\n    <Box\n      position=\"fixed\"\n      width={collapsed ? '100px' : '300px'}\n      height={collapsed ? '50px' : '200px'}\n      cursor={collapsed ? 'default' : 'move'}\n      onMouseDown={!collapsed ? handleMouseDown : null}\n      onMouseMove={!collapsed ? handleMouseMove : null}\n      onMouseUp={!collapsed ? handleMouseUp : null}\n      style={{\n        left: `${position.x}px`,\n        top: `${position.y}px`,\n        zIndex: 1000,\n      }}\n      border=\"2px solid gray\"\n      borderRadius=\"10px\"\n      overflow=\"hidden\"\n      boxShadow=\"lg\"\n      bg=\"gray.900\"\n    >\n      <Box position=\"absolute\" top=\"0\" right=\"0\">\n        <IconButton\n          icon={collapsed ? <ChevronUpIcon /> : <ChevronDownIcon />}\n          size=\"sm\"\n          onClick={toggleCollapse}\n          aria-label=\"Toggle collapse\"\n        />\n      </Box>\n\n      {!collapsed && (\n        <>\n          <video ref={videoElementRef} autoPlay muted style={{ width: '100%', height: '100%' }} />\n          <Box position=\"absolute\" bottom=\"0\" left=\"0\" p=\"2\" color=\"white\" bg=\"rgba(0, 0, 0, 0.5)\">\n            {emotionState.detectedEmotion && (\n              <div>Detected Emotion: {emotionState.detectedEmotion}</div>\n            )}\n          </Box>\n        </>\n      )}\n    </Box>\n  );\n};\n\nexport default DraggableVideoBox;","import React from 'react';\nimport { Radar } from 'react-chartjs-2';\nimport { Box } from '@chakra-ui/react';\nimport {\n    Chart as ChartJS,\n    RadialLinearScale,\n    PointElement,\n    LineElement,\n    Filler,\n    Tooltip,\n    Legend\n} from 'chart.js';\n\nChartJS.register(RadialLinearScale, PointElement, LineElement, Filler, Tooltip, Legend);\n\nconst EmotionRadarChart = ({ emotionIntensities }) => {\n    const data = {\n        labels: ['Joy', 'Anger', 'Disgust', 'Fear', 'Sadness', 'Surprise'],\n        datasets: [\n            {\n                label: 'Emotion Intensities',\n                data: [\n                    emotionIntensities.joy,\n                    emotionIntensities.anger,\n                    emotionIntensities.disgust,\n                    emotionIntensities.fear,\n                    emotionIntensities.sadness,\n                    emotionIntensities.surprise\n                ],\n                backgroundColor: 'rgba(34, 202, 236, 0.2)',\n                borderColor: 'rgba(34, 202, 236, 1)',\n                borderWidth: 2\n            }\n        ]\n    };\n\n    const options = {\n        scale: {\n            ticks: { beginAtZero: true, max: 1 },\n            angleLines: { display: true }\n        }\n    };\n\n    return (\n        <Box width=\"400px\" height=\"400px\" p={4}>\n            <Radar data={data} options={options} />\n        </Box>\n    );\n};\n\nexport default EmotionRadarChart;","import React, { useEffect, useRef } from 'react';\nimport useEmo from '../hooks/useEmo';\nimport DraggableVideoBox from './DraggableVideoBox';\nimport EmotionRadarChart from './EmotionRadarChart';\nimport GameText from './GameText';\n\nconst EmotionDetection = ({ onEmotionStateChange }) => {\n    const { emotionState, startEmoDetection, stopEmoDetection, videoElementRef } = useEmo();\n\n    // Start detection when component mounts and clean up when unmounting\n    useEffect(() => {\n        startEmoDetection();\n\n        return () => {\n            stopEmoDetection();\n        };\n    }, [startEmoDetection, stopEmoDetection]);\n\n    // Pass emotionState to parent via callback\n    useEffect(() => {\n        if (onEmotionStateChange) {\n            onEmotionStateChange(emotionState);  // Provide the updated emotion state\n        }\n    }, [emotionState, onEmotionStateChange]);\n\n    return (\n        <div>\n            <DraggableVideoBox videoElementRef={videoElementRef} emotionState={emotionState} />\n            <EmotionRadarChart emotionIntensities={emotionState.emotionIntensities} />\n            <GameText text={emotionState.detectedEmotion || ''} />\n        </div>\n    );\n};\n\nexport default EmotionDetection;","import { Box, Icon, Text } from '@chakra-ui/react';\nimport { FaCircle } from 'react-icons/fa';\n\nconst TrafficLightIndicator = ({ status }) => {\n  const colors = {\n    talking: 'red.500',\n    listening: 'green.500',\n    idle: 'yellow.500',\n  };\n\n  return (\n    <Box\n      position=\"fixed\"\n      zIndex=\"1000\"\n      bottom=\"20px\"\n      left=\"20px\"\n      bg=\"gray.700\"\n      borderRadius=\"md\"\n      p={4}\n      boxShadow=\"xl\"\n      userSelect=\"none\"\n    >\n      <Box display=\"flex\" flexDirection=\"column\" alignItems=\"center\">\n        <Icon as={FaCircle} color={colors[status]} boxSize={6} mb={2} />\n        <Text color=\"white\" fontWeight=\"bold\">\n          {status === 'talking' && 'Talking'}\n          {status === 'listening' && 'Listening'}\n          {status === 'idle' && 'Idle'}\n        </Text>\n      </Box>\n    </Box>\n  );\n};\n\nexport default TrafficLightIndicator;","import { useState, useCallback, useRef } from 'react';\nimport ConversationManager from './../VISOS/cognition/ConversationManager'; \n\nconst useConvo = (audioToText, voiceManager, conversationManager, textToSpeakGen, transcribedTextGen) => {\n    const [conversationState, setConversationState] = useState({\n        status: 'idle',\n        transcribedText: null,\n        speakingText: null,\n    });\n\n    // Invoke the generator functions to create iterators\n    const transcribedTextIterator = useRef(transcribedTextGen()).current;\n    const textToSpeakIterator = useRef(textToSpeakGen()).current;\n\n    const handleSpeakingText = useCallback(async (text) => {\n        if (!text || text.trim() === '') {\n            setConversationState((prev) => ({\n                ...prev,\n                status: 'listening',\n                speakingText: null,\n            }));\n            return;\n        }\n\n        setConversationState((prev) => ({\n            ...prev,\n            status: 'talking',\n            speakingText: text,\n        }));\n\n        await voiceManager.enqueueText(text);\n\n        setConversationState((prev) => ({\n            ...prev,\n            status: 'listening',\n            speakingText: null,\n        }));\n        startListening();\n    }, [voiceManager]);\n\n    const processTranscribedText = useCallback((transcribedText) => {\n        setConversationState((prev) => ({\n            ...prev,\n            status: 'thinking',\n            transcribedText,\n        }));\n\n        // Get the next feedback from the transcribedTextGenerator\n        const { value: feedback } = transcribedTextIterator.next(transcribedText);\n\n        if (feedback) {\n            handleSpeakingText(feedback).then(() => {\n                const { value: nextQuestion } = textToSpeakIterator.next();  // Iterate the generator\n                if (nextQuestion) {\n                    handleSpeakingText(nextQuestion);\n                } else {\n                    setConversationState((prev) => ({\n                        ...prev,\n                        status: 'idle',\n                        speakingText: null,\n                    }));\n                }\n            });\n        }\n    }, [handleSpeakingText, transcribedTextIterator, textToSpeakIterator]);\n\n    const startListening = useCallback(() => {\n        setConversationState((prev) => ({ ...prev, status: 'listening' }));\n\n        audioToText.startContinuousRecognition((transcribedText) => {\n            processTranscribedText(transcribedText);\n        });\n    }, [audioToText, processTranscribedText]);\n\n    const startConversation = useCallback(async () => {\n        const { value: firstQuestion } = textToSpeakIterator.next();\n\n        setConversationState((prev) => ({\n            ...prev,\n            status: 'talking',\n            speakingText: firstQuestion,\n        }));\n\n        await voiceManager.enqueueText(firstQuestion);\n\n        setConversationState((prev) => ({ ...prev, status: 'listening', speakingText: null }));\n        startListening();\n    }, [voiceManager, startListening, textToSpeakIterator]);\n\n    const stopConversation = useCallback(() => {\n        audioToText.stopRecognition();\n        voiceManager.stopSpeech();\n        setConversationState({ status: 'idle', transcribedText: null, speakingText: null });\n    }, [audioToText, voiceManager]);\n\n    return { conversationState, startConversation, stopConversation };\n};\n\nexport default useConvo;","import { useEffect } from 'react';\nimport { showHappy, showSad, showAngry, showFear, showSurprise, showDisgust } from '../VISOS/action/visualizers/basicEmotions';\n\nconst useMirroring = (animationManager, emotionState) => {\n    // Effect to mirror emotions based on the detected emotion state passed from EmotionDetection\n    useEffect(() => {\n        if (emotionState && emotionState.detectedEmotion) {\n            const { detectedEmotion, emotionIntensities } = emotionState;\n\n            console.log('Detected Emotion:', detectedEmotion);  // Debug log to track detected emotions\n            console.log('Emotion Intensity:', emotionIntensities[detectedEmotion]);  // Log the intensity of the detected emotion\n\n            // Map the detected emotion and its intensity to the agent's facial expressions\n            mirrorEmotionToAgent(detectedEmotion, emotionIntensities[detectedEmotion], animationManager);\n        }\n    }, [emotionState, animationManager]);\n\n    // Function to mirror the detected emotion to the agent based on intensity\n    const mirrorEmotionToAgent = (emotion, intensity, animationManager) => {\n        console.log(`Mirroring ${emotion} with intensity: ${intensity}`);  // Log the emotion being mirrored\n\n        // Apply slower reaction for negative emotions\n        let delay = emotion === 'joy' || emotion === 'surprise' ? 0 : 2000;\n\n        switch (emotion) {\n            case 'joy':\n                showHappy(animationManager, intensity * 100, 500);  // Scale the intensity for facial expressions\n                break;\n            case 'sadness':\n                setTimeout(() => {\n                    showSad(animationManager, intensity * 100, 500);\n                }, delay);\n                break;\n            case 'anger':\n                setTimeout(() => {\n                    showAngry(animationManager, intensity * 100, 700);\n                }, delay);\n                break;\n            case 'surprise':\n                showSurprise(animationManager, intensity * 100, 300);\n                break;\n            case 'fear':\n                setTimeout(() => {\n                    showFear(animationManager, intensity * 100, 700);\n                }, delay);\n                break;\n            case 'disgust':\n                setTimeout(() => {\n                    showDisgust(animationManager, intensity * 100, 700);\n                }, delay);\n                break;\n            default:\n                console.log('No valid emotion detected');\n                break;\n        }\n    };\n\n    return null;  // No need to return anything since this is a side-effect hook\n};\n\nexport default useMirroring;","export function showHappy(animationManager, intensity = 100, duration = 250) {\n    // Happy emotion using smile (AU12) and other happiness-related AUs\n    animationManager.scheduleChange(\"12\", intensity, duration, 0);  // Smile (AU12)\n    animationManager.scheduleChange(\"6\", intensity * 0.6, duration, 0);    // Cheek raise (AU6) for happiness\n}\n\nexport function showSad(animationManager, intensity = 100, duration = 500) {\n    // Sad emotion with frown (AU1) and related AUs for sadness\n    animationManager.scheduleChange(\"1\", intensity, duration, 0);    // Inner brow raise (AU1)\n    animationManager.scheduleChange(\"15\", intensity * 0.6, duration, 0);   // Lip corner depressor (AU15)\n}\n\nexport function showAngry(animationManager, intensity = 100, duration = 500) {\n    // Angry emotion with AU4 (brow lower) and AU7 (tight eyelids)\n    animationManager.scheduleChange(\"4\", intensity, duration, 0);   // Brow lower (AU4)\n    animationManager.scheduleChange(\"7\", intensity, duration, 0);   // Tighten eyelids (AU7)\n}\n\nexport function showFear(animationManager, intensity = 100, duration = 500) {\n    // Fear emotion using wide eyes (AU5) and tense mouth (AU20)\n    animationManager.scheduleChange(\"5\", intensity, duration, 0);   // Upper eyelid raise (AU5)\n    animationManager.scheduleChange(\"20\", intensity * 0.8, duration, 0);   // Lip stretch (AU20)\n}\n\nexport function showSurprise(animationManager, intensity = 100, duration = 250) {\n    // Surprise emotion with wide eyes (AU5) and raised eyebrows (AU2)\n    animationManager.scheduleChange(\"5\", intensity, duration, 0);   // Upper eyelid raise (AU5)\n    animationManager.scheduleChange(\"2\", intensity * 0.8, duration, 0);    // Outer brow raise (AU2)\n}\n\nexport function showDisgust(animationManager, intensity = 100, duration = 500) {\n    // Disgust emotion using AU9 (nose wrinkle) and AU10 (upper lip raise)\n    animationManager.scheduleChange(\"9\", intensity, duration, 0);    // Nose wrinkle (AU9)\n    animationManager.scheduleChange(\"10\", intensity * 0.8, duration, 0);   // Upper lip raise (AU10)\n}"],"names":["ConversationManager","constructor","bufferTime","arguments","length","undefined","audioToText","voiceManager","wordThreshold","this","isListening","startListening","Promise","resolve","reject","console","log","startContinuousRecognition","transcribedText","onerror","error","stopListening","stopRecognition","enqueueText","text","then","setTimeout","resumeListening","catch","detectInterruption","handleInterruption","split","userText","concat","stopSpeech","resumeListeningAfterResponse","setStatus","AudioToText","recognitionType","recognition","isRecognizing","isManuallyStopped","initRecognition","window","SpeechRecognition","webkitSpeechRecognition","interimResults","continuous","lang","onRecognizedCallback","onend","startRecognitionProcess","warn","finalTranscript","onresult","event","results","i","resultIndex","isFinal","push","transcript","trim","join","alert","start","stop","VideoToEmotion","videoElement","isProcessing","intervalId","initVideo","loadModels","MODEL_URL","process","faceapi","tinyFaceDetector","loadFromUri","faceLandmark68Net","faceExpressionNet","faceRecognitionNet","Error","startVideoStream","stream","navigator","mediaDevices","getUserMedia","video","srcObject","startContinuousDetection","onEmotionDetectedCallback","setInterval","async","detections","withFaceLandmarks","withFaceExpressions","emotions","expressions","stopDetection","clearInterval","stopVideoStream","getTracks","forEach","track","useEmo","emotionState","setEmotionState","useState","detectedEmotion","emotionIntensities","joy","anger","disgust","fear","sadness","surprise","status","videoToEmotion","useRef","current","videoElementRef","startEmoDetection","useCallback","prev","happy","angry","disgusted","fearful","sad","surprised","topEmotion","Object","keys","reduce","a","b","stopEmoDetection","_ref","position","setPosition","x","y","dragging","setDragging","collapsed","setCollapsed","_jsxs","Box","width","height","cursor","onMouseDown","onMouseMove","clientX","target","offsetWidth","clientY","offsetHeight","onMouseUp","handleMouseUp","windowWidth","innerWidth","newX","newY","innerHeight","style","left","top","zIndex","border","borderRadius","overflow","boxShadow","bg","children","_jsx","right","IconButton","icon","ChevronUpIcon","ChevronDownIcon","size","onClick","toggleCollapse","_Fragment","ref","autoPlay","muted","bottom","p","color","ChartJS","register","RadialLinearScale","PointElement","LineElement","Filler","Tooltip","Legend","data","labels","datasets","label","backgroundColor","borderColor","borderWidth","Radar","options","scale","ticks","beginAtZero","max","angleLines","display","onEmotionStateChange","useEffect","DraggableVideoBox","EmotionRadarChart","GameText","userSelect","flexDirection","alignItems","Icon","as","FaCircle","talking","listening","idle","boxSize","mb","Text","fontWeight","useConvo","conversationManager","textToSpeakGen","transcribedTextGen","conversationState","setConversationState","speakingText","transcribedTextIterator","textToSpeakIterator","handleSpeakingText","processTranscribedText","value","feedback","next","nextQuestion","startConversation","firstQuestion","stopConversation","useMirroring","animationManager","mirrorEmotionToAgent","emotion","intensity","delay","duration","scheduleChange","showHappy","showSad","showAngry","showSurprise","showFear","showDisgust"],"sourceRoot":""}