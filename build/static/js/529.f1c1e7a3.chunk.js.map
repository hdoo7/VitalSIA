{"version":3,"file":"static/js/529.f1c1e7a3.chunk.js","mappings":"uHAAe,MAAMA,EACjBC,WAAAA,GACIC,KAAKC,YAAc,KACnBD,KAAKE,aAAc,CACvB,CAEAC,oBAAAA,GACI,MAAM,4BAA6BC,QAKnCJ,KAAKC,YAAc,IAAIG,OAAOC,wBAC9BL,KAAKC,YAAYK,YAAa,EAC9BN,KAAKC,YAAYM,gBAAiB,EAClCP,KAAKC,YAAYO,KAAO,QAEjBC,QAAQC,YATXC,QAAQC,MAAM,iDACPH,QAAQI,OAAO,gCAS9B,CAEAC,0BAAAA,CAA2BC,GACvB,IAAIf,KAAKE,YAKT,OAAO,IAAIO,SAAQ,CAACC,EAASG,KACzBb,KAAKG,uBAAuBa,MAAK,KAC7BhB,KAAKC,YAAYgB,SAAYC,IACzB,MAAMC,EAAaC,MAAMC,KAAKH,EAAMI,SAC/BC,KAAIC,GAAUA,EAAO,GAAGL,aACxBM,KAAK,IACVd,QAAQe,IAAI,eAADC,OAAgBR,EAAWS,SACtCb,EAAqBI,EAAWS,OAAO,EAG3C5B,KAAKC,YAAY4B,QAAWX,IACxBP,QAAQC,MAAM,4BAA6BM,EAAMN,OACjDC,EAAOK,EAAMN,MAAM,EAGvBZ,KAAKC,YAAY6B,MAAQ,KACjB9B,KAAKE,aACLF,KAAKC,YAAY8B,OACrB,EAGJ/B,KAAKE,aAAc,EACnBF,KAAKC,YAAY8B,QACjBrB,GAAS,IACVsB,OAAMpB,IACLD,QAAQC,MAAM,yCAA0CA,GACxDC,EAAOD,EAAM,GACf,IA/BFD,QAAQsB,KAAK,kCAiCrB,CAEAC,eAAAA,GACQlC,KAAKC,cACLD,KAAKE,aAAc,EACnBF,KAAKC,YAAYkC,OACjBxB,QAAQe,IAAI,wBAEpB,E,yGC/DW,MAAMU,EACjBrC,WAAAA,GAAqD,IAAzCsC,EAAcC,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,GAAIG,EAAUH,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,IAC1CtC,KAAKqC,eAAiBjB,MAAMsB,QAAQL,GAC9BA,EAAed,KAAIoB,GAAUA,EAAOC,cAAchB,SAClD,GACN5B,KAAKyC,WAAaA,EAClBzC,KAAK6C,cAAgB,KACrB7C,KAAK8C,WAAa,EACtB,CAGAC,eAAAA,CAAgBC,GACZ,MAAMC,EAAYD,EAAKJ,cAAchB,OACrC,OAAO5B,KAAKqC,eAAea,MAAKP,GAAUM,EAAUE,SAASR,IACjE,CAGAS,gBAAAA,CAAiBJ,EAAMK,GACfrD,KAAK6C,eAAeS,aAAatD,KAAK6C,eAE1C7C,KAAK8C,WAAWS,KAAKP,GAGrBhD,KAAK6C,cAAgBW,YAAW,KAC5B,MAAMC,EAAYzD,KAAK8C,WAAWrB,KAAK,KACvCzB,KAAK8C,WAAa,GAElB,MAAMY,EAAiB1D,KAAK+C,gBAAgBU,GACxCJ,GACAA,EAASK,GAAkCD,EAC/C,GACDzD,KAAKyC,WACZ,CAGAkB,cAAAA,CAAeN,GACX,IAAKjD,OAAOwD,oBAAsBxD,OAAOC,wBAErC,YADAM,QAAQC,MAAM,2CAIlB,MACMX,EAAc,IADMG,OAAOwD,mBAAqBxD,OAAOC,yBAE7DJ,EAAYK,YAAa,EACzBL,EAAYM,gBAAiB,EAC7BN,EAAYO,KAAO,QAEnBP,EAAYgB,SAAYC,IACpB,MAAMC,EAAaD,EAAMI,QAAQJ,EAAMI,QAAQiB,OAAS,GAAG,GAAGpB,WAC9DR,QAAQe,IAAI,yBAA0BP,GAGtCnB,KAAKoD,iBAAiBjC,EAAYkC,EAAS,EAG/CpD,EAAY8B,QACZpB,QAAQe,IAAI,kDAChB,CAGAmC,aAAAA,GACQ7D,KAAK8D,oBACL9D,KAAK8D,kBAAkB3B,OACvBxB,QAAQe,IAAI,iDAEpB,ECVF,QAvDF,MACI3B,WAAAA,CAAYgE,GACV,IAAKA,GAA4B,kBAAXA,EACpB,MAAM,IAAIC,MAAM,4CAElBhE,KAAK+D,OAASA,EACd/D,KAAKiE,OAAS,4CAChB,CAQA,iBAAMC,CAAYlB,GAChB,MAAMmB,EAAU,CACdC,MAAO,gBACPC,SAAU,CACR,CAAEC,KAAM,SAAUC,QAJWjC,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,oCAKhC,CAAEgC,KAAM,OAAQC,QAASvB,IAE3BwB,WAAY,IACZC,YAAa,IAGf,IAAK,IAADC,EAAAC,EAAAC,EAAAC,EACF,MAAMC,QAAiBC,MAAM/E,KAAKiE,OAAQ,CACxCe,OAAQ,OACRC,QAAS,CACP,cAAgB,UAADtD,OAAY3B,KAAK+D,QAChC,eAAgB,oBAElBmB,KAAMC,KAAKC,UAAUjB,KAGvB,IAAKW,EAASO,GACZ,MAAM,IAAIrB,MAAM,mCAADrC,OAAoCmD,EAASQ,SAG9D,MACMC,EAA0B,QAAfb,SADEI,EAASU,QACHC,eAAO,IAAAf,GAAK,QAALC,EAAZD,EAAe,UAAE,IAAAC,GAAS,QAATC,EAAjBD,EAAmBe,eAAO,IAAAd,GAAS,QAATC,EAA1BD,EAA4BL,eAAO,IAAAM,OAAvB,EAAZA,EAAqCjD,OAEzD,IAAK2D,EACH,MAAM,IAAIvB,MAAM,+CAGlB,OAAOuB,CACT,CAAE,MAAO3E,GAEP,MADAD,QAAQC,MAAM,kCAAmCA,GAC3CA,CACR,CACF,G,sECjDJ,MA+BA,EA/B8B+E,IAAiB,IAAhB,OAAEL,GAAQK,EAOvC,OACEC,EAAAA,EAAAA,KAACC,EAAAA,GAAG,CACFC,SAAS,QACTC,OAAO,OACPC,OAAO,OACPC,KAAK,OACLC,GAAG,WACHC,aAAa,KACbC,EAAG,EACHC,UAAU,KACVC,WAAW,OAAMC,UAEjBC,EAAAA,EAAAA,MAACX,EAAAA,GAAG,CAACY,QAAQ,OAAOC,cAAc,SAASC,WAAW,SAAQJ,SAAA,EAC5DX,EAAAA,EAAAA,KAACgB,EAAAA,EAAI,CAACC,GAAIC,EAAAA,IAAUC,MAnBX,CACbC,QAAS,UACTC,UAAW,YACXC,KAAM,cAgBgC5B,GAAS6B,QAAS,EAAGC,GAAI,KAC3DZ,EAAAA,EAAAA,MAACa,EAAAA,EAAI,CAACN,MAAM,QAAQO,WAAW,OAAMf,SAAA,CACvB,YAAXjB,GAAwB,UACb,cAAXA,GAA0B,YACf,SAAXA,GAAqB,cAGtB,ECrBV,IAAIiC,EACAC,EACAC,EACAC,EACAC,GAAsB,EACtBC,GAAW,EACXC,EAAc,GACdC,EAAO,KACX,MAoBMC,EAA2BjD,IAC7B,MAAMkD,EAAS,GACf,IAAIC,EAAe,GAenB,OAbAnD,EAASoD,MAAM,KAAKC,SAAQC,IACpBH,EAAa1F,OAAS6F,EAAK7F,OAAS,EAxBrB,KAyBfyF,EAAOzE,KAAK0E,GACZA,EAAeG,GAEfH,IAAyC,IAAxBA,EAAa1F,OAAe,GAAK,KAAO6F,CAC7D,IAGAH,EAAa1F,OAAS,GACtByF,EAAOzE,KAAK0E,GAGTD,CAAM,EAIXK,EAAwBA,CAACrF,EAAMsF,EAAWC,KA1BtBvF,IACf6E,GAAe7E,EAAKG,SAAS0E,GA0BhCW,CAAiBxF,GACjBrC,QAAQe,IAAI,sCAIhBf,QAAQe,IAAI,qBAADC,OAAsBqB,IACjCuF,EAAM,CACFE,MAAO,mBACPC,YAAa1F,EACbsC,OAAQ,OACRqD,SAAU,MAGVf,IACAF,EAAakB,aACbN,EAAU,SAGdd,EAA2BqB,gBAAgB7F,GACtChC,MAAKQ,IAEGA,EAKDA,EAAOmB,SAAWgF,EAClBmB,EAAoBtH,EAAOmB,OAAQ2F,EAAWC,GACvC/G,EAAOuH,UAAYpB,EAC1BqB,EAAexH,EAAOuH,SAAUT,EAAWC,GAE3C5H,QAAQC,MAAM,4BAA6BY,GAT3Cb,QAAQC,MAAM,qCAUlB,IAEHoB,OAAMpB,IACHD,QAAQC,MAAM,yBAA0BA,EAAM,IAChD,EAIJkI,EAAsBA,CAACG,EAAeX,EAAWC,KACnDZ,GAAsB,EACtBE,EAAc,+BACdD,GAAW,EAGXJ,EAA2B3D,gBAE3B6D,EAAawB,YAAYrB,GACzBS,EAAU,WAEVC,EAAM,CACFE,MAAO,mBACPC,YAAY,cAAD/G,OAAgBsH,EAAa,oBACxC3D,OAAQ,UACRqD,SAAU,MAGdlB,EAAcvD,YAAY+E,EAAe,qBACpCjI,MAAK8D,IACF,GAAIA,EAAU,CACSiD,EAAwBjD,GAChCqD,SAAQ1E,IACfiE,EAAawB,YAAYzF,EAAU,IAEvCoE,EAAc/C,CAClB,CAEAwD,EAAU,QACVV,GAAW,EAGXJ,EAA2B2B,6BAA6Bb,GAAYtF,GAASqF,EAAsBrF,EAAMsF,EAAWC,IAAO,GAC7H,EAIJS,EAAiBA,CAACI,EAAcd,EAAWC,KAC7CX,GAAW,EACXJ,EAA2B3D,gBAE3ByE,EAAU,WAEVb,EAAcvD,YAAYkF,EAAc,qBACnCpI,MAAK8D,IACF,GAAIA,EAAU,CACSiD,EAAwBjD,GAChCqD,SAAQ1E,IACfiE,EAAawB,YAAYzF,EAAU,IAEvCoE,EAAc/C,CAClB,CAEAwD,EAAU,QACVV,GAAW,EAGXJ,EAA2B2B,6BAA6Bb,GAAYtF,GAASqF,EAAsBrF,EAAMsF,EAAWC,IAAO,GAC7H,EAIGxG,EAAQA,CAACsH,EAAkBC,EAAaC,KACjD,IAAKA,IAAiBA,EAAaC,QAE/B,YADA7I,QAAQC,MAAM,+BA9II6I,EAACJ,EAAkBC,KACzC,MAAMjH,EAAiBiH,EAAYjH,gBAAkB,GAC/C0B,EAASuF,EAAYvF,OAE3BwD,EAAc,IAAIzH,EAAAA,EAAY,aAC9B0H,EAA6B,IAAIpF,EAA2BC,EATvC,IASyEkF,GAC9FE,EAAgB,IAAIiC,EAAoB3F,GACxC2D,EAAeiC,EAAAA,EAAaC,YAAYP,EAAiB,EA2IzDI,CAAkBJ,EAAkBC,GAEpC,MAAMO,EAAUA,KACZ,MAAOvE,EAAQgD,IAAawB,EAAAA,EAAAA,UAAS,QAC/BvB,GAAQwB,EAAAA,EAAAA,KAUd,OARAC,EAAAA,EAAAA,YAAU,KACNxC,EAA2B7D,gBAAgBX,GAASqF,EAAsBrF,EAAMsF,EAAWC,KAEpF,KACHhB,EAAYrF,iBAAiB,IAElC,CAACoG,EAAWC,KAER3C,EAAAA,EAAAA,KAACqE,EAAqB,CAAC3E,OAAQA,GAAU,EAG/CwC,IACDA,EAAOoC,EAAAA,WAA0BX,EAAaC,UAGlD1B,EAAKqC,QAAOvE,EAAAA,EAAAA,KAACiE,EAAO,IAAI,EAIf1H,EAAOA,KACZoF,GACAA,EAAYrF,kBAEZwF,GACAA,EAAakB,aAEbd,IACAA,EAAKsC,UACLtC,EAAO,KACX,C","sources":["VISOS/perception/audio/AudioToText.js","VISOS/perception/audio/TextToListenerWithFollowUp.js","VISOS/cognition/TextToGptReconciler.js","components/TrafficLightIndicator.jsx","modules/chat.js"],"sourcesContent":["export default class AudioToText {\n    constructor() {\n        this.recognition = null;\n        this.isListening = false;\n    }\n\n    initializeRecognizer() {\n        if (!('webkitSpeechRecognition' in window)) {\n            console.error('Web Speech API not supported in this browser.');\n            return Promise.reject('Web Speech API not supported');\n        }\n\n        this.recognition = new window.webkitSpeechRecognition();\n        this.recognition.continuous = true;\n        this.recognition.interimResults = false;\n        this.recognition.lang = 'en-US'; // You can change the language as needed\n\n        return Promise.resolve();\n    }\n\n    startContinuousRecognition(onRecognizedCallback) {\n        if (this.isListening) {\n            console.warn('Recognition is already running.');\n            return;\n        }\n\n        return new Promise((resolve, reject) => {\n            this.initializeRecognizer().then(() => {\n                this.recognition.onresult = (event) => {\n                    const transcript = Array.from(event.results)\n                        .map(result => result[0].transcript)\n                        .join('');\n                    console.log(`RECOGNIZED: ${transcript.trim()}`);\n                    onRecognizedCallback(transcript.trim());\n                };\n\n                this.recognition.onerror = (event) => {\n                    console.error('Speech recognition error:', event.error);\n                    reject(event.error);\n                };\n\n                this.recognition.onend = () => {\n                    if (this.isListening) {\n                        this.recognition.start(); // Restart if recognition ends\n                    }\n                };\n\n                this.isListening = true;\n                this.recognition.start();\n                resolve();\n            }).catch(error => {\n                console.error('Error initializing speech recognition:', error);\n                reject(error);\n            });\n        });\n    }\n\n    stopRecognition() {\n        if (this.recognition) {\n            this.isListening = false;\n            this.recognition.stop();\n            console.log('Recognition stopped.');\n        }\n    }\n}","export default class TextToListenerWithFollowUp {\n    constructor(triggerPhrases = [], bufferTime = 1000) {\n        this.triggerPhrases = Array.isArray(triggerPhrases)\n            ? triggerPhrases.map(phrase => phrase.toLowerCase().trim())\n            : [];\n        this.bufferTime = bufferTime;\n        this.debounceTimer = null;\n        this.utterances = [];\n    }\n\n    // Detect if the spoken text contains a trigger phrase\n    detectKeyPhrase(text) {\n        const lowerText = text.toLowerCase().trim();\n        return this.triggerPhrases.find(phrase => lowerText.includes(phrase));\n    }\n\n    // Process each utterance separately and debounce to avoid concatenation\n    processUtterance(text, callback) {\n        if (this.debounceTimer) clearTimeout(this.debounceTimer);\n\n        this.utterances.push(text);\n\n        // Debounce and process after the buffer time\n        this.debounceTimer = setTimeout(() => {\n            const utterance = this.utterances.join(' ');\n            this.utterances = []; // Clear the buffer\n\n            const detectedPhrase = this.detectKeyPhrase(utterance);\n            if (callback) {\n                callback(detectedPhrase ? detectedPhrase : utterance);\n            }\n        }, this.bufferTime);\n    }\n\n    // Start listening and processing the audio stream\n    startListening(callback) {\n        if (!window.SpeechRecognition && !window.webkitSpeechRecognition) {\n            console.error('SpeechRecognition API is not supported.');\n            return;\n        }\n\n        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n        const recognition = new SpeechRecognition();\n        recognition.continuous = true;\n        recognition.interimResults = false;\n        recognition.lang = 'en-US';\n\n        recognition.onresult = (event) => {\n            const transcript = event.results[event.results.length - 1][0].transcript;\n            console.log('Transcribed utterance:', transcript);\n\n            // Process each utterance individually\n            this.processUtterance(transcript, callback);\n        };\n\n        recognition.start();\n        console.log('TextToListenerWithFollowUp started listening...');\n    }\n\n    // Stop listening for the audio stream\n    stopListening() {\n        if (this.speechRecognition) {\n            this.speechRecognition.stop();\n            console.log('TextToListenerWithFollowUp stopped listening.');\n        }\n    }\n}","class TextToGptReconciler {\n    constructor(apiKey) {\n      if (!apiKey || typeof apiKey !== 'string') {\n        throw new Error('A valid OpenAI API key must be provided.');\n      }\n      this.apiKey = apiKey;\n      this.apiUrl = 'https://api.openai.com/v1/chat/completions';\n    }\n  \n    /**\n     * Processes the provided text by sending it to the OpenAI API.\n     * @param {string} text - The input text to process.\n     * @param {string} instruction - Instruction or system prompt for the AI.\n     * @returns {Promise<string>} - The GPT response.\n     */\n    async processText(text, instruction = 'Answer in a professional manner:') {\n      const payload = {\n        model: 'gpt-3.5-turbo',  // You can switch this model based on your requirement\n        messages: [\n          { role: 'system', content: instruction },\n          { role: 'user', content: text }\n        ],\n        max_tokens: 150,  // Adjust as necessary\n        temperature: 0.7\n      };\n  \n      try {\n        const response = await fetch(this.apiUrl, {\n          method: 'POST',\n          headers: {\n            'Authorization': `Bearer ${this.apiKey}`,\n            'Content-Type': 'application/json'\n          },\n          body: JSON.stringify(payload)\n        });\n  \n        if (!response.ok) {\n          throw new Error(`API request failed with status: ${response.status}`);\n        }\n  \n        const data = await response.json();\n        const gptResponse = data.choices?.[0]?.message?.content?.trim();\n  \n        if (!gptResponse) {\n          throw new Error('Failed to get a valid response from the API');\n        }\n  \n        return gptResponse;\n      } catch (error) {\n        console.error('Error processing text with GPT:', error);\n        throw error;\n      }\n    }\n  }\n  \n  export default TextToGptReconciler;","import { Box, Icon, Text } from '@chakra-ui/react';\nimport { FaCircle } from 'react-icons/fa';\n\nconst TrafficLightIndicator = ({ status }) => {\n  const colors = {\n    talking: 'red.500',\n    listening: 'green.500',\n    idle: 'yellow.500',\n  };\n\n  return (\n    <Box\n      position=\"fixed\"\n      zIndex=\"1000\" // High z-index to ensure visibility\n      bottom=\"20px\"\n      left=\"20px\"\n      bg=\"gray.700\"\n      borderRadius=\"md\"\n      p={4}\n      boxShadow=\"xl\"\n      userSelect=\"none\"\n    >\n      <Box display=\"flex\" flexDirection=\"column\" alignItems=\"center\">\n        <Icon as={FaCircle} color={colors[status]} boxSize={6} mb={2} />\n        <Text color=\"white\" fontWeight=\"bold\">\n          {status === 'talking' && 'Talking'}\n          {status === 'listening' && 'Listening'}\n          {status === 'idle' && 'Idle'}\n        </Text>\n      </Box>\n    </Box>\n  );\n};\n\nexport default TrafficLightIndicator;","import ReactDOMClient from 'react-dom/client';\nimport React, { useState, useEffect } from 'react';\nimport { useToast } from '@chakra-ui/react';\nimport AudioToText from './../VISOS/perception/audio/AudioToText';\nimport TextToListenerWithFollowUp from './../VISOS/perception/audio/TextToListenerWithFollowUp';\nimport TextToGptReconciler from './../VISOS/cognition/TextToGptReconciler';\nimport VoiceManager from './../VISOS/action/verbalizers/VoiceManager';\nimport TrafficLightIndicator from '../components/TrafficLightIndicator';\n\nlet audioToText;\nlet textToListenerWithFollowUp;\nlet gptReconciler;\nlet voiceManager;\nlet conversationStarted = false;\nlet speaking = false;\nlet agentSpeech = '';  // Track agent's last spoken phrase\nlet root = null;\nconst listenBufferTime = 2000;  // Buffer time to resume listening after speaking\nconst maxUtteranceLength = 120;  // Max characters per utterance\n\n// Initialize necessary modules\nconst initializeModules = (animationManager, appSettings) => {\n    const triggerPhrases = appSettings.triggerPhrases || [];\n    const apiKey = appSettings.apiKey;\n\n    audioToText = new AudioToText('webspeech');\n    textToListenerWithFollowUp = new TextToListenerWithFollowUp(triggerPhrases, listenBufferTime, audioToText);\n    gptReconciler = new TextToGptReconciler(apiKey);\n    voiceManager = VoiceManager.getInstance(animationManager);\n};\n\n// Ensure the agent does not respond to its own speech\nconst shouldIgnoreText = (text) => {\n    return agentSpeech && text.includes(agentSpeech);  // Prevent response to agent's own speech\n};\n\n// Break response into smaller utterances\nconst breakResponseIntoChunks = (response) => {\n    const chunks = [];\n    let currentChunk = '';\n\n    response.split(' ').forEach(word => {\n        if (currentChunk.length + word.length + 1 > maxUtteranceLength) {\n            chunks.push(currentChunk);\n            currentChunk = word;\n        } else {\n            currentChunk += (currentChunk.length === 0 ? '' : ' ') + word;\n        }\n    });\n\n    if (currentChunk.length > 0) {\n        chunks.push(currentChunk);\n    }\n\n    return chunks;\n};\n\n// Handle transcribed text and start/stop listening\nconst handleTranscribedText = (text, setStatus, toast) => {\n    if (shouldIgnoreText(text)) {\n        console.log('Ignoring agentâ€™s own speech.');\n        return;\n    }\n\n    console.log(`Transcribed text: ${text}`);\n    toast({\n        title: 'Transcribed text',\n        description: text,\n        status: 'info',\n        duration: 2000,\n    });\n\n    if (speaking) {\n        voiceManager.stopSpeech();\n        setStatus('idle');\n    }\n\n    textToListenerWithFollowUp.listenForStream(text)\n        .then(result => {\n            // Ensure result is not null or undefined\n            if (!result) {\n                console.error(\"No result returned from listening.\");\n                return;\n            }\n\n            if (result.phrase && !conversationStarted) {\n                handleTriggerPhrase(result.phrase, setStatus, toast);\n            } else if (result.followUp && conversationStarted) {\n                handleFollowUp(result.followUp, setStatus, toast);\n            } else {\n                console.error(\"Unexpected result format:\", result);\n            }\n        })\n        .catch(error => {\n            console.error(\"Error processing text:\", error);\n        });\n};\n\n// Handle the trigger phrase\nconst handleTriggerPhrase = (triggerPhrase, setStatus, toast) => {\n    conversationStarted = true;\n    agentSpeech = \"Let me check that for you...\";\n    speaking = true;\n\n    // Stop listening to avoid transcribing agent's speech\n    textToListenerWithFollowUp.stopListening();\n\n    voiceManager.enqueueText(agentSpeech);\n    setStatus('talking');\n\n    toast({\n        title: 'Trigger Detected',\n        description: `You said: \"${triggerPhrase}\". Processing...`,\n        status: 'success',\n        duration: 3000,\n    });\n\n    gptReconciler.processText(triggerPhrase, 'Answer seriously:')\n        .then(response => {\n            if (response) {\n                const utterances = breakResponseIntoChunks(response);\n                utterances.forEach(utterance => {\n                    voiceManager.enqueueText(utterance);  // Speak each chunk\n                });\n                agentSpeech = response;\n            }\n\n            setStatus('idle');\n            speaking = false;\n\n            // Resume listening after the agent finishes speaking\n            textToListenerWithFollowUp.resumeListeningAfterResponse(setStatus, (text) => handleTranscribedText(text, setStatus, toast));\n        });\n};\n\n// Handle follow-up text\nconst handleFollowUp = (followUpText, setStatus, toast) => {\n    speaking = true;\n    textToListenerWithFollowUp.stopListening();\n\n    setStatus('talking');\n\n    gptReconciler.processText(followUpText, 'Answer seriously:')\n        .then(response => {\n            if (response) {\n                const utterances = breakResponseIntoChunks(response);\n                utterances.forEach(utterance => {\n                    voiceManager.enqueueText(utterance);\n                });\n                agentSpeech = response;\n            }\n\n            setStatus('idle');\n            speaking = false;\n\n            // Ensure continuous listening\n            textToListenerWithFollowUp.resumeListeningAfterResponse(setStatus, (text) => handleTranscribedText(text, setStatus, toast));\n        });\n};\n\n// Start function to initialize and render the ChatApp using ReactDOM\nexport const start = (animationManager, appSettings, containerRef) => {\n    if (!containerRef || !containerRef.current) {\n        console.error('Invalid container reference');\n        return;\n    }\n\n    initializeModules(animationManager, appSettings);\n\n    const ChatApp = () => {\n        const [status, setStatus] = useState('idle');\n        const toast = useToast();\n\n        useEffect(() => {\n            textToListenerWithFollowUp.startListening((text) => handleTranscribedText(text, setStatus, toast));\n\n            return () => {\n                audioToText.stopRecognition();\n            };\n        }, [setStatus, toast]);\n\n        return <TrafficLightIndicator status={status} />;\n    };\n\n    if (!root) {\n        root = ReactDOMClient.createRoot(containerRef.current);\n    }\n\n    root.render(<ChatApp />);\n};\n\n// Stop function to unmount ChatApp and stop processes\nexport const stop = () => {\n    if (audioToText) {\n        audioToText.stopRecognition();\n    }\n    if (voiceManager) {\n        voiceManager.stopSpeech();\n    }\n    if (root) {\n        root.unmount();\n        root = null;\n    }\n};"],"names":["AudioToText","constructor","this","recognition","isListening","initializeRecognizer","window","webkitSpeechRecognition","continuous","interimResults","lang","Promise","resolve","console","error","reject","startContinuousRecognition","onRecognizedCallback","then","onresult","event","transcript","Array","from","results","map","result","join","log","concat","trim","onerror","onend","start","catch","warn","stopRecognition","stop","TextToListenerWithFollowUp","triggerPhrases","arguments","length","undefined","bufferTime","isArray","phrase","toLowerCase","debounceTimer","utterances","detectKeyPhrase","text","lowerText","find","includes","processUtterance","callback","clearTimeout","push","setTimeout","utterance","detectedPhrase","startListening","SpeechRecognition","stopListening","speechRecognition","apiKey","Error","apiUrl","processText","payload","model","messages","role","content","max_tokens","temperature","_data$choices","_data$choices$","_data$choices$$messag","_data$choices$$messag2","response","fetch","method","headers","body","JSON","stringify","ok","status","gptResponse","json","choices","message","_ref","_jsx","Box","position","zIndex","bottom","left","bg","borderRadius","p","boxShadow","userSelect","children","_jsxs","display","flexDirection","alignItems","Icon","as","FaCircle","color","talking","listening","idle","boxSize","mb","Text","fontWeight","audioToText","textToListenerWithFollowUp","gptReconciler","voiceManager","conversationStarted","speaking","agentSpeech","root","breakResponseIntoChunks","chunks","currentChunk","split","forEach","word","handleTranscribedText","setStatus","toast","shouldIgnoreText","title","description","duration","stopSpeech","listenForStream","handleTriggerPhrase","followUp","handleFollowUp","triggerPhrase","enqueueText","resumeListeningAfterResponse","followUpText","animationManager","appSettings","containerRef","current","initializeModules","TextToGptReconciler","VoiceManager","getInstance","ChatApp","useState","useToast","useEffect","TrafficLightIndicator","ReactDOMClient","render","unmount"],"sourceRoot":""}